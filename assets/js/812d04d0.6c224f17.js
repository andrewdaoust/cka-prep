"use strict";(self.webpackChunkcka_prep_2=self.webpackChunkcka_prep_2||[]).push([[6254],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),h=c(a),u=o,m=h["".concat(l,".").concat(u)]||h[u]||p[u]||r;return a?n.createElement(m,s(s({ref:t},d),{},{components:a})):n.createElement(m,s({ref:t},d))}));function u(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,s=new Array(r);s[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var c=2;c<r;c++)s[c]=a[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},3492:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return d},default:function(){return h}});var n=a(7462),o=a(3366),r=(a(7294),a(3905)),s=["components"],i={id:"chapter04",title:"Kubernetes Architecture"},l=void 0,c={unversionedId:"fundamentals/chapter04",id:"fundamentals/chapter04",title:"Kubernetes Architecture",description:"Course Reading",source:"@site/docs/fundamentals/chapter04.md",sourceDirName:"fundamentals",slug:"/fundamentals/chapter04",permalink:"/cka-prep/docs/fundamentals/chapter04",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fundamentals/chapter04.md",tags:[],version:"current",frontMatter:{id:"chapter04",title:"Kubernetes Architecture"},sidebar:"tutorialSidebar",previous:{title:"Installation and Configuration",permalink:"/cka-prep/docs/fundamentals/chapter03"},next:{title:"APIs and Access",permalink:"/cka-prep/docs/fundamentals/chapter05"}},d=[{value:"Course Reading",id:"course-reading",children:[{value:"Learning objectives",id:"learning-objectives",children:[],level:3},{value:"Main components",id:"main-components",children:[],level:3},{value:"Control plane node",id:"control-plane-node",children:[{value:"kube-apiserver",id:"kube-apiserver",children:[],level:4},{value:"kube-scheduler",id:"kube-scheduler",children:[],level:4},{value:"etcd database",id:"etcd-database",children:[],level:4},{value:"Other agents",id:"other-agents",children:[],level:4}],level:3},{value:"Worker nodes",id:"worker-nodes",children:[],level:3},{value:"Kubelet",id:"kubelet",children:[],level:3},{value:"Operators",id:"operators",children:[],level:3},{value:"Service operators",id:"service-operators",children:[],level:3},{value:"Pods",id:"pods",children:[],level:3},{value:"Rewrite legacy applications",id:"rewrite-legacy-applications",children:[{value:"High replacement costs",id:"high-replacement-costs",children:[],level:4},{value:"Large outage effect",id:"large-outage-effect",children:[],level:4},{value:"Lack of flexibility",id:"lack-of-flexibility",children:[],level:4}],level:3},{value:"Containers",id:"containers",children:[],level:3},{value:"Init containers",id:"init-containers",children:[],level:3},{value:"Component review",id:"component-review",children:[],level:3},{value:"API call flow",id:"api-call-flow",children:[],level:3},{value:"Node",id:"node",children:[],level:3},{value:"Single IP per pod",id:"single-ip-per-pod",children:[],level:3},{value:"Container to outside path",id:"container-to-outside-path",children:[],level:3},{value:"Services",id:"services",children:[],level:3},{value:"Networking setup",id:"networking-setup",children:[],level:3},{value:"CNI network configuration file",id:"cni-network-configuration-file",children:[],level:3},{value:"Pod-to-pod communication",id:"pod-to-pod-communication",children:[],level:3},{value:"Mesos",id:"mesos",children:[],level:3}],level:2},{value:"Lab Exercises",id:"lab-exercises",children:[{value:"Lab 4.1 - Basic node maintenance",id:"lab-41---basic-node-maintenance",children:[{value:"Back up etcd",id:"back-up-etcd",children:[],level:4},{value:"Upgrading the cluster",id:"upgrading-the-cluster",children:[],level:4}],level:3},{value:"Lab 4.2 - Working with CPU and memory constraints",id:"lab-42---working-with-cpu-and-memory-constraints",children:[],level:3},{value:"Lab 4.3 - Resource limits for a namesapce",id:"lab-43---resource-limits-for-a-namesapce",children:[],level:3}],level:2},{value:"Knowledge check",id:"knowledge-check",children:[],level:2}],p={toc:d};function h(e){var t=e.components,i=(0,o.Z)(e,s);return(0,r.kt)("wrapper",(0,n.Z)({},p,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"course-reading"},"Course Reading"),(0,r.kt)("h3",{id:"learning-objectives"},"Learning objectives"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Discuss main components of a Kubernetes cluster"),(0,r.kt)("li",{parentName:"ul"},"Learn details about the master agent kube-apiserver"),(0,r.kt)("li",{parentName:"ul"},"Learn how etcd keeps the cluster state and configuration"),(0,r.kt)("li",{parentName:"ul"},"Study kubelet local agent"),(0,r.kt)("li",{parentName:"ul"},"Learn how controllers manage the cluster state"),(0,r.kt)("li",{parentName:"ul"},"Learn what a Pod is to the cluster"),(0,r.kt)("li",{parentName:"ul"},"Learn more about the network configuration of the cluster"),(0,r.kt)("li",{parentName:"ul"},"Discuss Kubernetes services")),(0,r.kt)("h3",{id:"main-components"},"Main components"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Control plane and worker nodes"),(0,r.kt)("li",{parentName:"ul"},"Operators"),(0,r.kt)("li",{parentName:"ul"},"Services"),(0,r.kt)("li",{parentName:"ul"},"Pods and containers"),(0,r.kt)("li",{parentName:"ul"},"Namespaces and quotas"),(0,r.kt)("li",{parentName:"ul"},"Network and policies"),(0,r.kt)("li",{parentName:"ul"},"Storage")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Kubernetes architecture",src:a(8361).Z,width:"728",height:"361"})),(0,r.kt)("p",null,"A Kubernetes cluster is made of two main parts, the cp nodes and the worker nodes.  The cluster is controlled through API calls to operators, both intrnally and from external traffic.  Most of thr processes that occur in the cluster run in containers."),(0,r.kt)("h3",{id:"control-plane-node"},"Control plane node"),(0,r.kt)("p",null,"The cp node are responsible for running the manager processes for the clsuter.  As Kubernetes continues to mature, more components are developed for dedicated handling of certain needs.  An example of this is the ",(0,r.kt)("strong",{parentName:"p"},"cloud-controler-manager")," which handle interactions with tooling that sits on top of Kubernetes, but these processes were once part of the ",(0,r.kt)("strong",{parentName:"p"},"kube-controller-mamager"),"."),(0,r.kt)("p",null,"It is common place to have add-ons for a production cluster that handle specific tasks, like DNS services or cluster-level logging and monitoring."),(0,r.kt)("p",null,"Conceptually, the cp is the various pods responsible for ensuring the current cluster state matches the desired state. For clusters build with ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm"),", kubelet processes are managed by systemd and once running it will start every pod found in ",(0,r.kt)("inlineCode",{parentName:"p"},"/etc/kubernetes/manifests/"),"."),(0,r.kt)("h4",{id:"kube-apiserver"},"kube-apiserver"),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"kube-apiserver")," is responsible for all calls to the cluster (internal and external) and is central to the operation of the cluster.  It accepts and validates all action of the cluster, as well as validating and configuring all data for API objects, and handles all REST operations.  Thw kube-apiserver is also the only part of the clsuter that has connection with the ",(0,r.kt)("strong",{parentName:"p"},"etcd")," database."),(0,r.kt)("p",null,"As of v1.18, there is a beta Konnectivity service that separtes user traffic from the internal server traffic. Until this feature was added, all traffic was not segregated which had performance, capacity, and security ramifications."),(0,r.kt)("h4",{id:"kube-scheduler"},"kube-scheduler"),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"kube-scheduler")," is in charge of scheduling which nodes will host which Pods.  The scheduler will view available resources to bind, and will try and retry to deploy the Pods depnding on availability and success."),(0,r.kt)("p",null,"Custom schedulers can also be used to use a different algorithm than the default methodology.  There also several ways to affect how pods get applied with the default algorithm, such as taints and tolerations, as well as wth metadata labels"),(0,r.kt)("p",null,"The code for the scheduler can be found ",(0,r.kt)("a",{parentName:"p",href:"https://raw.githubusercontent.com/kubernetes/kubernetes/master/pkg/scheduler/scheduler.go"},"here")," to view the specifics of what is going on under the hood."),(0,r.kt)("h4",{id:"etcd-database"},"etcd database"),(0,r.kt)("p",null,"Cluster state, networking, and all other persistent information is all stored in an ",(0,r.kt)("strong",{parentName:"p"},"etcd")," database (which is a b+tree key-value store).  Values are always appended to the end and previous copies of data are marked for removal by compaction processes.  It works with curl abd other HTTP libraries."),(0,r.kt)("p",null,"All requests to update values travel via the ",(0,r.kt)("strong",{parentName:"p"},"kube-apiserver"),", which then passes along the request to ",(0,r.kt)("strong",{parentName:"p"},"etcd"),".  For simultaneous requests, the first request would update the database and the second request would not have the same version number and the kube-apiserver would respond with a 409 error to the requester.  There is no further logic past the denail, so it is on the client to act upon a denial to push an update to the database."),(0,r.kt)("p",null,"etcd consists of a ",(0,r.kt)("em",{parentName:"p"},"Leader")," database with possible ",(0,r.kt)("em",{parentName:"p"},"followers"),", or non-voting ",(0,r.kt)("em",{parentName:"p"},"Learners")," who are joining the cluster.  They have ongoing communication to determine which will be the Leader or to determine a new one in case of a failure."),(0,r.kt)("p",null,"etcd is very fast and has the potential for durability, although there are some hiccups with new tools (kubeadm included) and features (like upgrading the entire cluster).  While most Kubernetes objects are deisgned to be transient microservices, etcd is the exception and cannot be terminated without any concern like most others.  Because it is the persistent state of the cluster care must be taken to protect and secure it.  Before any upgrades or maintenance. etcd should be backed up.  The ",(0,r.kt)("inlineCode",{parentName:"p"},"etcdctl")," command allows for this with ",(0,r.kt)("inlineCode",{parentName:"p"},"snapshot save")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"snapshot restore"),"."),(0,r.kt)("h4",{id:"other-agents"},"Other agents"),(0,r.kt)("p",null,"The ",(0,r.kt)("strong",{parentName:"p"},"kube-controller-manager")," is a core control loop deamon, which along with the kube-apiserver, determines the state of the cluster.  When the state does not match, the kube-controller-manager contacts the controller responsible for getting the state to where it need to be.  There are several operators in use (endpoint, namespace, replication) and the list has grown as Kubernetes matures."),(0,r.kt)("p",null,"In beta since v1.11, the ",(0,r.kt)("strong",{parentName:"p"},"cloud-controller-manager"),", or ",(0,r.kt)("strong",{parentName:"p"},"ccm"),", interacts with agents outside the cluster.  It's processes where once managed by the kube-control-manager.  Splitting out these processes allows for faster changes without changing the core Kubernetes control process.  Each kubelet needs to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"--cloud-provider-external")," settings passed to the binary.  A custom ccm can also be developed and dployed as a daemonset as an in-tree or out-of-tree deployment.  It is an optional component and takes some steps to enable. You can learn about them ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/"},"here"),"."),(0,r.kt)("p",null,"Depending on the networking plugin, there could be varous pods to handle network traffic.  To handle all DNS queries, Kubernetes service discovery, and other function ",(0,r.kt)("strong",{parentName:"p"},"CoreDNS")," server has replaced ",(0,r.kt)("strong",{parentName:"p"},"kube-dns"),".  The ability to use chained plugins allows for the server to be easily extensible."),(0,r.kt)("h3",{id:"worker-nodes"},"Worker nodes"),(0,r.kt)("p",null,"All the nodes run the kubelet and kube-proxy as well as a container engine.  Other management daemons watch these agents or enhance functionality for services not yet included by Kubernetes."),(0,r.kt)("p",null,"The kubelet interacts with the container engine and makes sure the containers that should be running are.  The kube-proxy handles all the networking to the containers using iptable entries.  The kube-proxy also has a userspace mode to monitor Services and Endpoints using a random port to proxy any traffic using ipvs. Depnding on the network plugin chosen, you may also see pods for the plugin."),(0,r.kt)("p",null,"Each node could run in a different engine and it is likely that Kubernetes will support more container runtimes as it continues to mature."),(0,r.kt)("p",null,"While not part of a typicall installation, some users will user supervisord to monitor the kubelet and docker processes and restart them in they fail and log events.  Kuberentes does not yet have cluster-wide logging, and instead the CNCF project ",(0,r.kt)("a",{parentName:"p",href:"https://www.fluentd.org/"},"Fluentd")," is used, which when implement correctly, gives a logging layer for the cluster which can filter, buffer, and route messages."),(0,r.kt)("p",null,"Metric on a cluster-wide scale is another lacking area of Kubernetes.  There is a metrics-server SIG which gives basic functionality to collect CPU and memory utilization of a node or pod.  For more complete metrics, many use the ",(0,r.kt)("a",{parentName:"p",href:"https://prometheus.io/"},"Prometheus")," project."),(0,r.kt)("h3",{id:"kubelet"},"Kubelet"),(0,r.kt)("p",null,"The kubelet systemd process accepts the API calls for Pod specification (known as a ",(0,r.kt)("inlineCode",{parentName:"p"},"PodSpec")," which is a JOSN or YAML file that describes a pod) and works to configure the local node to meet the specification.  The kublet is also responsible for the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Mounting volumes to Pod"),(0,r.kt)("li",{parentName:"ul"},"Downloading secrots"),(0,r.kt)("li",{parentName:"ul"},"Passing requests to local continaer engine"),(0,r.kt)("li",{parentName:"ul"},"Repoting Pod and node status to cluster")),(0,r.kt)("p",null,"The kubelet also calls other components like the Topology Manager, whih uses ",(0,r.kt)("em",{parentName:"p"},"hints")," to conifigure topology-aware resource NUMA assignments for CPU and hardware acceleration. This is off by default as it is an alpha feature."),(0,r.kt)("h3",{id:"operators"},"Operators"),(0,r.kt)("p",null,"Operators (also known as controllers or watch-loops) as important concepts for orchestration.  Kubernetes has many that come with it, and it can be extedned with custom operators. Operators work as two parts, an ",(0,r.kt)("em",{parentName:"p"},"Informer")," and a downstream store in the form of a DeltaFIFO queue.  The loop process recieves an ",(0,r.kt)("inlineCode",{parentName:"p"},"obj")," (object) comprised of an array of deltas from the queue. The operator's logic then creates/modifies some object until it matches the specification, as long as the delta is not a ",(0,r.kt)("inlineCode",{parentName:"p"},"Deleted")," type."),(0,r.kt)("p",null,"The Informer calls on the API server to request the object state and the data is then cached to minimize transactions to the API.  There is also a ",(0,r.kt)("em",{parentName:"p"},"SharedInformer")," for objects used by multiple others which creates a shared cache fo multiple request."),(0,r.kt)("p",null,"There is also a ",(0,r.kt)("em",{parentName:"p"},"Workqueue")," which takes keys to had out tasks to workers.  They use standard Goland work queues (rate limiting, delayed, and time queue) typically."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"endpoints"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"namespace"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"serviceaccounts")," operators each manges the Pod resources thay are named after.  "),(0,r.kt)("p",null,"Deployments manage replicaSets. replicaSets manage Pods that share the same podSpec.  Each Pod managed by a replicaSet is called a replica."),(0,r.kt)("h3",{id:"service-operators"},"Service operators"),(0,r.kt)("p",null,"A ",(0,r.kt)("em",{parentName:"p"},"service")," is a operator which listens to the ",(0,r.kt)("em",{parentName:"p"},"endpoint")," operator to provide a persistent IP for a Pod.  This is needed due to the decoupling of objects and agents which allows for flexible connection of resources and reconnection on replacement."),(0,r.kt)("p",null,"The service operator sensd messages via the kube-apiserver to the kube-proxy of each node and also to the netowkr plugin."),(0,r.kt)("p",null,"Services also handle access policies for inbound requests which help contro resources and security."),(0,r.kt)("p",null,"The key take aways on services are:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Connects Pods"),(0,r.kt)("li",{parentName:"ul"},"Exposes Pods to Internet"),(0,r.kt)("li",{parentName:"ul"},"Decouples settings"),(0,r.kt)("li",{parentName:"ul"},"Defines access policies for Pods")),(0,r.kt)("h3",{id:"pods"},"Pods"),(0,r.kt)("p",null,"While point of Kubernetes is to orchestrate containers, the smallest unit we deal with is a Pod. Pods can contain multiple containers and due to sharing resources, the Pod is designed to run a one-process-per-container architecture."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Pod of whales (to stick with the Docker theme)"),(0,r.kt)("li",{parentName:"ul"},"Peas in a pod")),(0,r.kt)("p",null,"The containers of the Pod start up in parallel and there is not a way to determine which becomes available first. Using ",(0,r.kt)("inlineCode",{parentName:"p"},"InitContainers")," can order their startup to some extent.  To support the single process running in a container, the other containers may handle logging, proxies, or special adapters."),(0,r.kt)("p",null,"Most network plugins give the Pod one IP to share across the containers. For the containers to talk to one another they must do so via IPC, the loopback interface, or with a shared file system."),(0,r.kt)("p",null,"One of the most common cases for having more than one container in a Pod is for logging.  This would be an example of a ",(0,r.kt)("em",{parentName:"p"},"sidecar")," container, which is dedicated to performing a helper task."),(0,r.kt)("h3",{id:"rewrite-legacy-applications"},"Rewrite legacy applications"),(0,r.kt)("p",null,"It can be costly to move legacy applications to work as containerized, decoupled microservices.  Can the application be containerized as is or does it need to be rewritten?  Here's some of the challanges of legacy infrastructure and disgn practices and how cloud native design patterns help solve these problems"),(0,r.kt)("h4",{id:"high-replacement-costs"},"High replacement costs"),(0,r.kt)("p",null,"With Kubernetes, here is a low cost to replace broken components since by its nature, it is designed to be decoupled.  When comparing to legacy, this can be more of a challange to fix a small issue with a monolithic application as the whole pplication will likely need to be updated to update the one problem area."),(0,r.kt)("h4",{id:"large-outage-effect"},"Large outage effect"),(0,r.kt)("p",null,"In a legacy monolithic system, an issue with one area can bring down the whole application, while with Kubernetes decoupled and transient architecture, and outage with one microservice will not affect the entire app (depending on application and the function of the microservice experiencing the outage)"),(0,r.kt)("h4",{id:"lack-of-flexibility"},"Lack of flexibility"),(0,r.kt)("p",null,"If you are experienceing loads and need to scale resources this will affect the entire application for monolithicly designed software. This is in contrast to Kubernetes, where if just one area needs to be scaled this can be done by just updating the replicaSet to manage more replicas."),(0,r.kt)("p",null,"With a monolithic application you are also limited to running one version unless you want to dedicate the resources to run an entirely new version.  Using Kubernetes you could run different versions of specific components of your application which give the application much more flexibility."),(0,r.kt)("h3",{id:"containers"},"Containers"),(0,r.kt)("p",null,"While we cannot manipulate container directly with Kubernetes we do have control over their resources. In the PodSpec you can pass parameters to controll the resources that container may use."),(0,r.kt)("p",null,"An example"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'resources:\n  limits: \n    cpu: "1"\n    memory: "4Gi" \n  requests:\n    cpu: "0.5"\n    memory: "500Mi"\n')),(0,r.kt)("p",null,"Resourcing can also be controlled using a ",(0,r.kt)("em",{parentName:"p"},"ResourceQuota")," object to set hard and sot limits in a namespace.  The ResourceQuota object allows management of more than just CPU and memory and can apply to several object."),(0,r.kt)("p",null,"As a beta feature in v1.12, the ",(0,r.kt)("em",{parentName:"p"},"scopeSelector")," field in the quota spec to run a pod at a specific priority if it has the appropriate ",(0,r.kt)("em",{parentName:"p"},"priorityClassName")," in the PodSpec."),(0,r.kt)("h3",{id:"init-containers"},"Init containers"),(0,r.kt)("p",null,"An ",(0,r.kt)("strong",{parentName:"p"},"init container")," can be used to help order the startup of containers in a Pod. Init container must complete before any app containers can run, and will continue to restart until it completes if it fails."),(0,r.kt)("p",null,"They can have different settings for use of storage and security, and therefore utility commands can be used which the app would not be allowed to. They can also contain code or utilities the app does not."),(0,r.kt)("div",{className:"admonition admonition-important alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"TODO")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"What would be a good example for the use case of an init container"))),(0,r.kt)("p",null,"Here is an example of the definition of an ",(0,r.kt)("inlineCode",{parentName:"p"},"initContainer")," that requires the ",(0,r.kt)("inlineCode",{parentName:"p"},"ls")," command to succeed before starting a database."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"spec:\n  containers:\n  - name: main-app\n    image: databaseD \n  initContainers:\n  - name: wait-database\n    image: busybox\n    command: ['sh', '-c', 'until ls /db/dir ; do sleep 5; done; ']\n")),(0,r.kt)("p",null,"Other options to determine startup order are ",(0,r.kt)("em",{parentName:"p"},"LivenessProbes"),", ",(0,r.kt)("em",{parentName:"p"},"ReadinessProbes"),", and ",(0,r.kt)("em",{parentName:"p"},"StatefulSets")," but they add complexity."),(0,r.kt)("h3",{id:"component-review"},"Component review"),(0,r.kt)("p",null,"Now that we have an understanding of the components that make up Kubernetes, lets review the architecture again."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"kubernetes architecture review",src:a(7049).Z,width:"761",height:"401"})),(0,r.kt)("p",null,"We can see that all the components talk to the kube-apiserver, and only it will talk to the etcd database."),(0,r.kt)("p",null,"In green, we can also see some commands that will help interact some of the other components."),(0,r.kt)("p",null,"Specifically there is the ",(0,r.kt)("inlineCode",{parentName:"p"},"etcdctl")," command to work with etcd directly, and the ",(0,r.kt)("inlineCode",{parentName:"p"},"calicoctl")," command to get networking information from Calico. The primary Calico component on each node also has a daemon, called Felix. It is responsible for monitoring and managing the network interface, route programming, ACL configuration and state reporting."),(0,r.kt)("p",null,"On the diagram is also ",(0,r.kt)("strong",{parentName:"p"},"BIRD"),", which is a dynamic IP routing daemon that is utilized by Felix. It reads the state of the routing and distributes it to the other nodes of the cluster. This allows a client to connect to any node and eventually the desired container's workload even when it is not the original node contacted."),(0,r.kt)("h3",{id:"api-call-flow"},"API call flow"),(0,r.kt)("p",null,"Imagine a scenario where you want to create a new deployment on the Kubernetes cluster.  To do this maybe you run something like, 'kubectl create deploy test1 --image=httpd`. This request goes to the kube-apiserver and the API calls on etcd to persist the request.  "),(0,r.kt)("p",null,"The kube-controller-manager then requests the kube-apiserver to see if there has been any change in the spec.  In this case, there has been a change because of the request coming in. The kube-apiserver then responds back to the kube-controller-manager with the new spec. Now the kube-controller-mamnger requests the status of this spec to see if it even exists. The kube-apiserver responds with information saying the deployment does not exist so the kube-controller-manager requests the kube-apiserver to create it. The kube-apiserver creates the spec and persists the information into etcd."),(0,r.kt)("p",null,"Then the same process to create the deployment is repeated to create the replicaSet. Then the prcoess again repeats to determine the existence of a Pod."),(0,r.kt)("p",null,"Once the pod is created and its state is persisted into etcd, the kube-apiserver calls on the kube-scheduler to determine which node will recieve this new podSpec. The kube-scheduler is always requesting information on the states of the worker nodes that is supplied by the kubelets. Once the scheduler has the information it needs, it responds with the information on whcih node the Pod should be sent to. The kube-apiserver then sends the information to the kubelet of that node the scheduler picked."),(0,r.kt)("p",null,"Networking information is sent to each of the kube-proxies by the kube-apiserver for each node so that each node (cp or worker) knows of this new networking configuration."),(0,r.kt)("p",null,"Finally the kubelet downloads all the config maps and secrets, and mounts any storage needed. Once the resources are available, the kubelet sends a request to the local container engine to create the Pod. The engine then returns the information to the kubelet, which in turns responds back to the kube-apiserver, which then persists the information to etcd."),(0,r.kt)("h3",{id:"node"},"Node"),(0,r.kt)("p",null,"A node is an API object that represents a machine instance.  The instance is created outside the cluster, has the necessary software installed, and then has its information ingested to the kube-apiserver to created the API object so the cluster can communicate with the new node."),(0,r.kt)("p",null,"Control plane nodes must be running on Linux instances, while worker nodes can also run on Windows Server 2019."),(0,r.kt)("p",null,"Currently, cp nodes can be created with ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm init")," and worker nodes joined with ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm join"),".  The future of ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm")," promises features of being able to join secondary cp nodes or etcd nodes as well. There are other tools available to provision nodes of a cluster as well."),(0,r.kt)("p",null,"When the kube=apiserver cannot communicate with the kubelet of a node for over 5 minutes, the default ",(0,r.kt)("inlineCode",{parentName:"p"},"NodeLease")," schedules the node to be deleted and the ",(0,r.kt)("inlineCode",{parentName:"p"},"NodeStatus"),' will no longer be "ready". Pods are evicted once connection to the cluster via the kube-apiserver is re-established. The cluster does not forcibly remove and reschedule them.'),(0,r.kt)("p",null,"The node objects exist in the ",(0,r.kt)("inlineCode",{parentName:"p"},"kube-node-lease")," namespace.  Nodes can be removed with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete node <node-name>\n")),(0,r.kt)("p",null,"This will remove the node from the kube-apiserver and cause the pods to evacuate.  Running ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm reset")," then removes the cluster specific information.  Removing iptables may be necessary as well if the node is to be re-used."),(0,r.kt)("p",null,"Running "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl describe node <node-name>\n")),(0,r.kt)("p",null,"will let you check the CPU usage, memory usage, requests, limits, capcity, pods allowed, current pods, among a few other things."),(0,r.kt)("h3",{id:"single-ip-per-pod"},"Single IP per pod"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"pod networking",src:a(8913).Z,width:"360",height:"394"})),(0,r.kt)("p",null,"The image shows an example of a pod with two volumes, 1 and 2, and three containers, A, B, and the ",(0,r.kt)("em",{parentName:"p"},"pause container"),". This pause container is used to get the an IP address and to share this network namespace with the other containers within the pod."),(0,r.kt)("p",null,"For the pods to communicate with one another they can use"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"the loopback interface"),(0,r.kt)("li",{parentName:"ul"},"file writes to a common file system"),(0,r.kt)("li",{parentName:"ul"},"interprocess communication (IPC)")),(0,r.kt)("p",null,"There is also a network plugin from HPE Labs that allows for multiple IPs per pod but this has not been adopted elsewhere."),(0,r.kt)("p",null,"Starting as an alpha feature from v1.16 you can use IPv4 or IPv6 for pods and services.  Current versions require creating the network for each address family separately when creating a service."),(0,r.kt)("h3",{id:"container-to-outside-path"},"Container to outside path"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"container external facing IP",src:a(4156).Z,width:"617",height:"695"})),(0,r.kt)("p",null,"In this example the two container share an IP address and a namespace. These are configured by kubectl working with the kube-proxy.  The IP is assigned to the pod before the containers are started and inserted into them on start up. The container has an interfact like ",(0,r.kt)("inlineCode",{parentName:"p"},"eth0@tun10"),", and the IP is set for the pod's life."),(0,r.kt)("p",null,"The endpoint is created at the same time as the service. Notice that the endpoint is the pod IP with a port number.  The service connects the network traffic from a high number port to the endpoint using iptables along with ipvs."),(0,r.kt)("p",null,"The kube-controller-manager's watch loops monitors the need for creating or deleting any services and endpoints."),(0,r.kt)("h3",{id:"services"},"Services"),(0,r.kt)("p",null,"Services are used to connect Pods to one another or to external traffic."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"service diagram",src:a(6698).Z,width:"1184",height:"1306"})),(0,r.kt)("p",null,"In the example, there is a primary and sidecar container, App and Logger respectively.  There is also the pause container that reserves the IP address in the namespace.  This container isn't seen within Kubernetes, but you could view it using ",(0,r.kt)("inlineCode",{parentName:"p"},"docker")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"crictl"),'.  We also see some ClusterIPs, used to connect to either a NodePort for outside the cluster, an IngressController or proxy, or to connect to another "backend" pod.'),(0,r.kt)("h3",{id:"networking-setup"},"Networking setup"),(0,r.kt)("p",null,"From a networking perspective, each pod can be treated as a virtual machine of physical hosts.  The network must assign an IP to each Pod and provide routes between all Pods on any nodes."),(0,r.kt)("p",null,"There are three main networking challenges in a container orchestration system:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Coupled container-to-container communications (solved with Pod concept)"),(0,r.kt)("li",{parentName:"ul"},"Pod-to-pod communication"),(0,r.kt)("li",{parentName:"ul"},"External-to-pod communication (solved by services concept)")),(0,r.kt)("p",null,"Kubernetes, then expects the networking configuration to handle the pod-to-pod is available, it will no do it for you."),(0,r.kt)("p",null,"Detailed explaination of the Kubernetes networking model can be found ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/cluster-administration/networking/"},"here")," in the official docs."),(0,r.kt)("p",null,"One of the lead developers of Kubernetes has also created ",(0,r.kt)("a",{parentName:"p",href:"https://speakerdeck.com/thockin/illustrated-guide-to-kubernetes-networking"},"this useful description")," of Kubernetes networking."),(0,r.kt)("h3",{id:"cni-network-configuration-file"},"CNI network configuration file"),(0,r.kt)("p",null,"To provide networking, Kubernetes has begun standardizing on the Container Networking Interface (CNI) spec. Since v1.6.0, ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm"),"'s goal was to use CNI."),(0,r.kt)("p",null,"CNI is an emerging spec with libraries that allow plug-ins to be written to container networking and remove allocated resources when containers are deleted.  CNI is language agnostic and provides a common interface for various networking solutions and container runtimes.  There are many plugins such as ones from Amazon ECS, SR-IOV, Cloud Foundry, and many more.  With CNI you can write network configuration files:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "cniVersion": "0.2.0",\n  "name": "mynet",\n  "type": "bridge",\n  "bridge": "cni0",\n  "isGateway": true,\n  "ipMasq": true,\n  "ipam": {\n    "type": "host-local",\n    "subnet": "10.22.0.0/16",\n    "routes": [\n      { "dst": "0.0.0.0/0" }\n    ]\n  }\n}\n')),(0,r.kt)("p",null,"The example configuration file creates a standard Linux bridge named ",(0,r.kt)("inlineCode",{parentName:"p"},"cni0")," which will give out IP address in the subnet ",(0,r.kt)("inlineCode",{parentName:"p"},"10.22.0.0/16"),". This bridge plug-in will configure network interfaces in the correct namespaces so that the container network is defined properly."),(0,r.kt)("p",null,"More info can be found in the ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/containernetworking/cni"},"CNI Github repo")," ",(0,r.kt)("inlineCode",{parentName:"p"},"README"),"."),(0,r.kt)("h3",{id:"pod-to-pod-communication"},"Pod-to-pod communication"),(0,r.kt)("p",null,"The CNI can be used to configure the network of a pod and provide a single IP address. It does not address the issue of pod-to-pod communication though."),(0,r.kt)("p",null,"Kubernetes requires the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"All pods can communicate with one another across all nodes"),(0,r.kt)("li",{parentName:"ul"},"All nodes can communicate with all pods"),(0,r.kt)("li",{parentName:"ul"},"No network address translation (NAT)")),(0,r.kt)("p",null,"Basically all IP (nodes and Pods) are routable without NAT. This is achievable at the physical network infrastructure level when you have access to is (e.g with GKE), or via a software defined overlay like many of the networking interfaces previously dicussed (Weave, Flannel, Calico, Romana, etc)."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/cluster-administration/networking/"},"These Kubernetes docs")," or this ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/cluster-administration/addons/"},"list")," provide a more complete list."),(0,r.kt)("h3",{id:"mesos"},"Mesos"),(0,r.kt)("p",null,"At a high level, there is not much difference between Kubernetes and other clustered orchestration systems."),(0,r.kt)("p",null,"Most have some sort of central managerthat exposes an API, something to schedule and place workloads on cluster nodes, and some persistent layer for the cluster state."),(0,r.kt)("p",null,"For example, here is a comparison of Kubernetes and Mesos."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Mesos architecture",src:a(92).Z,width:"836",height:"563"})),(0,r.kt)("p",null,"Overall the architecture is similar, just different technologies might be used for different components.  In Mesos, the persistence layer is implemented with Zookeeper, while Kubernetes uses etcd."),(0,r.kt)("p",null,"Similar comparisons could be made for systems like OpenStack or CloudStack."),(0,r.kt)("p",null,"Kubernetes sets itself apart with features targetting fault-tolerance, self-discovery, and scaling. It is also differentiated with it purely API-driven mindset."),(0,r.kt)("h2",{id:"lab-exercises"},"Lab Exercises"),(0,r.kt)("h3",{id:"lab-41---basic-node-maintenance"},"Lab 4.1 - Basic node maintenance"),(0,r.kt)("p",null,"This lab focuses on backing up etcd and updating the Kubernetes version on the cp and worker nodes."),(0,r.kt)("h4",{id:"back-up-etcd"},"Back up etcd"),(0,r.kt)("p",null,"The upgrade process for Kubernetes has become more stable but it is still good practice to backup the lcuster state before trying to back it up. There are many tools for managing and backing up etcd, and each has a distinct process for backing up and restoring. We'll use the included ",(0,r.kt)("inlineCode",{parentName:"p"},"snapshot")," command for ",(0,r.kt)("inlineCode",{parentName:"p"},"etcdctl"),". All the following commands in this section should be run on the cp node."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Find the data directory for the etcd daemon, which can be found in the manifest.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo grep data-dir /etc/kubernetes/manifests/etcd.yaml\n")),(0,r.kt)("p",null,"The response should be something like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"    - --data-dir=/var/lib/etcd\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Log into the etcd container")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n kube-system exec -it etcd-<Tab> -- sh\n")),(0,r.kt)("p",null,"This will open up a shell on the container.  Now have a look at the commands etcdctl offers with "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"etcdctl -h\n")),(0,r.kt)("p",null,"For TLS to work some files need to be passed to etcdctl. The etcd image is fairly minimal so many useful commands are not available.  To find the necessary files, it is important to remember the path ",(0,r.kt)("strong",{parentName:"p"},(0,r.kt)("inlineCode",{parentName:"strong"},"/etc/kubernetes/pki/etcd")),".  ",(0,r.kt)("inlineCode",{parentName:"p"},"cd")," to this location and list the files with ",(0,r.kt)("inlineCode",{parentName:"p"},"echo")," since ",(0,r.kt)("inlineCode",{parentName:"p"},"ls")," will not be available."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cd /etc/kubernetes/pki/etcd\necho *\n")),(0,r.kt)("p",null,"You should get a response similar to this from ",(0,r.kt)("inlineCode",{parentName:"p"},"echo *")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"ca.crt ca.key healthcheck-client.crt healthcheck-client.keypeer.crt peer.key server.crt server.key\n")),(0,r.kt)("p",null,"You can avoid having to type out each of these keys, especially in a limited shell environment using environment parameters. Log out with ",(0,r.kt)("inlineCode",{parentName:"p"},"exit")," and pass the paths as necessary. An example is in the next step."),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Check database health with the loopback IP on port ",(0,r.kt)("inlineCode",{parentName:"li"},"2379"),". This will need the peer cert and key and the certificate authority as environemtn variables.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl -n kube-system exec -it etcd-<Tab> -- sh \\\n  -c "ETCDCTL_API=3 \\\n  ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\\n  ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\\n  ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\\n  etcdctl endpoint health"\n')),(0,r.kt)("p",null,"You should get a response like this"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"127.0.0.1:2379 is healthy: successfully committed proposal: took = 14.813924ms\n")),(0,r.kt)("p",null,"Before continueing to the next step, let's break down the command we just used.  First is the ",(0,r.kt)("inlineCode",{parentName:"p"},"kubectl -n kube-system exec -it etcd-<Tab> -- sh")," part, which like the step before is used to open a shell in the container. On the next line is the ",(0,r.kt)("inlineCode",{parentName:"p"},"-c")," flag which is used with ",(0,r.kt)("inlineCode",{parentName:"p"},"sh")," to tell it to execute the following command. The command comes next, which is wrapped in the double quotes. First, we specify our environment variables:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ETCDCTL_API")," to specify the API version to use"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ETCDCTL_CACERT")," to specify the path to the certificate authority"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ETCDCTL_CERT")," to specify the path to the peer cert"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"ETCDCTL_KEY")," to specify the path to the peer key")),(0,r.kt)("p",null,"The last part is the actual command to check the health, which is ",(0,r.kt)("inlineCode",{parentName:"p"},"etcdctl endpoint health"),"."),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Determine the number of databases hat are part of the cluster. 3 and 5 are common in production clusters to meet 50%+1 for quorum.  For the leanring environment, there will be only one.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl -n kube-system exec -it etcd-ip-<Tab> -- sh \\\n  -c "ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\\n  ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\\n  ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key \\\n  etcdctl --endpoints=https://127.0.0.1:2379 member list"\n')),(0,r.kt)("p",null,"You should get a response like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"d4e6be3f3fdd0375, started, <hostname>, https://<host IP>:2380, https://<host IP>:2379, false\n")),(0,r.kt)("p",null,"By appending ",(0,r.kt)("inlineCode",{parentName:"p"},"-w table")," to the previous command you can also view this information in a table."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"+------------------+---------+-----------------+---------------------------+---------------------------+------------+\n|        ID        | STATUS  |      NAME       |        PEER ADDRS         |       CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+-----------------+---------------------------+---------------------------+------------+\n| d4e6be3f3fdd0375 | started |   <hostname>    |  https://<host IP>:2380   |  https://<host IP>:2379   |      false |\n+------------------+---------+-----------------+---------------------------+---------------------------+------------+\n")),(0,r.kt)("ol",{start:5},(0,r.kt)("li",{parentName:"ol"},"After figuring out the number of databases and the health we can back up etcd.  We'll use the ",(0,r.kt)("inlineCode",{parentName:"li"},"snapshot")," argument to save the file to ",(0,r.kt)("inlineCode",{parentName:"li"},"/var/lib/etcd"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'kubectl -n kube-system exec -it etcd-<Tab> -- sh \\\n  -c "ETCDCTL_API=3 \\\n  ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt \\\n  ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt \\\n  ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key  \\\n  etcdctl --endpoints=https://127.0.0.1:2379 \\\n  snapshot save /var/lib/etcd/snapshot.db "\n')),(0,r.kt)("p",null,"And you'll get a response similar to "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'{"level":"info","ts":1648840656.8329804,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/var/lib/etcd/snapshot.db.part"}\n{"level":"info","ts":"2022-04-01T19:17:36.839Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}\n{"level":"info","ts":1648840656.841397,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://127.0.0.1:2379"}\n{"level":"info","ts":"2022-04-01T19:17:36.890Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}\n{"level":"info","ts":1648840656.900381,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://127.0.0.1:2379","size":"4.0 MB","took":0.067326355}\n{"level":"info","ts":1648840656.9014206,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/var/lib/etcd/snapshot.db"}\nSnapshot saved at /var/lib/etcd/snapshot.db\n')),(0,r.kt)("p",null,"After creating the snapshot, verify it is there with "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo ls -l /var/lib/etcd/\n")),(0,r.kt)("p",null,"And the response should look like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"total 3916\ndrwx------ 4 root root    4096 Apr  1 17:47 member\n-rw------- 1 root root 4001824 Apr  1 19:17 snapshot.db\n")),(0,r.kt)("ol",{start:6},(0,r.kt)("li",{parentName:"ol"},"Now backup the snapshot and the other information used to create it.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"mkdir $HOME/backup\nsudo cp /var/lib/etcd/snapshot.db $HOME/backup/snapshot.db-$(date +%m-%d-%y)\nsudo cp /root/kubeadm-config.yaml $HOME/backup/\nsudo cp -r /etc/kubernetes/pki/etcd $HOME/backup/\n")),(0,r.kt)("p",null,"It is good practice to crete snapshots on a regular basis, using a cronjob or some other scheduling tool to creat them."),(0,r.kt)("p",null,"When using the ",(0,r.kt)("inlineCode",{parentName:"p"},"snapshot restore")," functionality, the database cannot be in use.  In an HA cluster, the control plane would be removed and replaced and a restore would not be needed.  Official documentation on restoring etcd can be found in the ",(0,r.kt)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#restoring-an-etcd-cluster"},"Kubernetes docs here")),(0,r.kt)("h4",{id:"upgrading-the-cluster"},"Upgrading the cluster"),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"'kubeadm' only supports updating versions 1.n.x to 1.n+1.x and 1.n.x to 1.n.y, where (y > x)."),(0,r.kt)("p",{parentName:"div"},"For example, if the cluster is built with ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm")," 1.22.1, to upgrade to the next major version you would need to go to 1.23.1, and not 1.23.4.  You could however go from 1.22.1 to 1.22.4"))),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Upgrade the package metadata and list the available versions of kubeadm that can be installed.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt update\nsudo apt-cache madison kubeadm\n")),(0,r.kt)("p",null,"You should get something like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"}," kubeadm |  1.22.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.22.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.22.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.22.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm | 1.21.11-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm | 1.21.10-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.9-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.8-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.7-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.3-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.2-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.1-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n kubeadm |  1.21.0-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Remove the hold on ",(0,r.kt)("inlineCode",{parentName:"li"},"kubeadm"),", install the next version of ",(0,r.kt)("inlineCode",{parentName:"li"},"kubeadm"),", and then add the hold back to the package to prevent unintended updates.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-mark unhold kubeadm\nsudo apt-get install -y kubeadm=1.22.1-00\nsudo apt-mark hold kubeadm\n")),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"In ",(0,r.kt)("a",{parentName:"p",href:"/cka-prep/docs/fundamentals/chapter03#lab-31---install-kubernetes"},"chapter 3")," we installed ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm=1.21.1-00")," so we must go to ",(0,r.kt)("inlineCode",{parentName:"p"},"kubeadm=1.22.1-00")))),(0,r.kt)("p",null,"Then check the version to confir the update was successful."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubeadm version\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'kubeadm version: &version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.1", GitCommit:"632ed300f2c34f6d6d15ca4cef3d3c7073412212", GitTreeState:"clean", BuildDate:"2021-08-19T15:44:22Z", GoVersion:"go1.16.7", Compiler:"gc", Platform:"linux/amd64"}\n')),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Prepare the cp node for updates. To do this we first need to evict as many pods as possible. By nature, daemonsets are on every node, and some, like the Calico ones, must remain.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl drain <node name> --ignore-daemonsets\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Once drained, you can upgrade the node. First you will want to see the upgrade plan. You will see different versions you can upgrade to but remember you will want to go from 1.n.x to 1.n+1.x, so for example, from 1.21.1 to 1.22.1. The upgrade plan will give you a detailed outline of the changes the upgrade will make.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo kubeadm upgrade plan\n")),(0,r.kt)("p",null,"Once you have reviewed the plan you can apply the upgrade. The command will give you a preview and ask you to confirm you want to upgrade. Review the preview and confirm to upgrade the node to the new version."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo kubeadm upgrade apply v1.22.1\n")),(0,r.kt)("ol",{start:5},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Check your CNI to get a compatible version with the new cluster version")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Now check the node status. It should be ",(0,r.kt)("inlineCode",{parentName:"p"},"SchedulingDisabled"),". Run"))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")),(0,r.kt)("p",null,"and you should get a response like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME              STATUS                     ROLES                  AGE   VERSION\ncp-node           Ready,SchedulingDisabled   control-plane,master   42d   v1.21.1\nworker-node       Ready                      <none>                 40d   v1.21.1\n")),(0,r.kt)("p",null,"We also need to update the other software and restart the daemons. First release the holds we set in the last lab, then upgrade both packages and add the hold back."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-mark unhold kubelet kubectl\nsudo apt-get install -y kubelet=1.22.1-00 kubectl=1.22.1-00\nsudo apt-mark hold kubelet kubectl\n")),(0,r.kt)("p",null,"Finally, restart the daemons."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo systemctl daemon-reload\nsudo systemctl restart kubelet\n")),(0,r.kt)("p",null,"Now get the nodes again to verify the cp has been upgraded with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")),(0,r.kt)("p",null,"and you should see something like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME              STATUS                     ROLES                  AGE   VERSION\ncp-node           Ready,SchedulingDisabled   control-plane,master   42d   v1.22.1\nworker-node       Ready                      <none>                 40d   v1.21.1\n")),(0,r.kt)("ol",{start:7},(0,r.kt)("li",{parentName:"ol"},"Now we can make the cp node available to the scheduler again.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl uncordon <node name>\n")),(0,r.kt)("p",null,"and verify the ready status with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")),(0,r.kt)("p",null,"and you should no longer see the ",(0,r.kt)("inlineCode",{parentName:"p"},"SchedulingDisabled")," as the ",(0,r.kt)("inlineCode",{parentName:"p"},"STATUS"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME              STATUS                     ROLES                  AGE   VERSION\ncp-node           Ready                      control-plane,master   42d   v1.22.1\nworker-node       Ready                      <none>                 40d   v1.21.1\n")),(0,r.kt)("ol",{start:8},(0,r.kt)("li",{parentName:"ol"},"Now we must update the worker node.")),(0,r.kt)("div",{className:"admonition admonition-important alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"important")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"Most of the next commands will need to be run on the ",(0,r.kt)("strong",{parentName:"p"},"worker node"),". There are a few commands that need to be run on the cp node.  ",(0,r.kt)("em",{parentName:"p"},"Commands that need to be run on the cp node will be in these blocks"),"."))),(0,r.kt)("p",null,"First, unhold, upgrade, and rehold kubeadm."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-mark unhold kubeadm\nsudo apt-get update\nsudo apt-get install -y kubeadm=1.22.1-00\nsudo apt-mark hold kubeadm\n")),(0,r.kt)("ol",{start:9},(0,r.kt)("li",{parentName:"ol"},"Next, on the cp node, drain the worker node.")),(0,r.kt)("div",{className:"admonition admonition-important alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"important")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl drain <worker node name> --ignore-daemonsets\n")))),(0,r.kt)("ol",{start:10},(0,r.kt)("li",{parentName:"ol"},"Now, on the worker upgrade the node.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo kubeadm upgrade node\n")),(0,r.kt)("ol",{start:11},(0,r.kt)("li",{parentName:"ol"},"Now unhold, upgrade, and rehold the other software.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"sudo apt-mark unhold kubelet kubectl\nsudo apt-get install -y kubelet=1.22.1-00 kubectl=1.22.1-00\nsudo apt-mark hold kubelet kubectl\n")),(0,r.kt)("ol",{start:12},(0,r.kt)("li",{parentName:"ol"},"Restart the deamons for the upgrade to take effect.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl daemon-reload\nsudo systemctl restart kubelet\n")),(0,r.kt)("ol",{start:13},(0,r.kt)("li",{parentName:"ol"},"Now, back on the cp, check the node status.")),(0,r.kt)("div",{className:"admonition admonition-important alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"important")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get nodes\n")))),(0,r.kt)("p",null,"You should see the worker needs to be uncordoned now to allow the scheduler to use it."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME              STATUS                     ROLES                  AGE   VERSION\ncp-node           Ready                      control-plane,master   42d   v1.22.1\nworker-node       Ready,SchedulingDisabled   <none>                 40d   v1.22.1\n")),(0,r.kt)("p",null,"Uncorden the worker node and check the status to make sure both nodes are ",(0,r.kt)("inlineCode",{parentName:"p"},"Ready"),"."),(0,r.kt)("div",{className:"admonition admonition-important alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"important")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl uncordon <worker node name>\nkubectl get node\n")))),(0,r.kt)("p",null,"Once both nodes are ready the statuses should look like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME          STATUS   ROLES                  AGE   VERSION\ncp-node       Ready    control-plane,master   42d   v1.22.1\nworker-node   Ready    <none>                 40d   v1.22.1\n")),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"This process was run through again to get to ",(0,r.kt)("inlineCode",{parentName:"p"},"v1.23.1")," to match the course notes, swapping anywhere it says ",(0,r.kt)("inlineCode",{parentName:"p"},"1.22.1")," for ",(0,r.kt)("inlineCode",{parentName:"p"},"1.23.1"),"."))),(0,r.kt)("h3",{id:"lab-42---working-with-cpu-and-memory-constraints"},"Lab 4.2 - Working with CPU and memory constraints"),(0,r.kt)("p",null,"This isction of the lab focuses on ",(0,r.kt)("em",{parentName:"p"},"resource limits"),", ",(0,r.kt)("em",{parentName:"p"},"namespaces"),", and more complex deployments.  We will again use the cluster we have been building up over the past lab exercises. SSH into the cp node and run all the commands for this section there."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"First, we will deploy a container called ",(0,r.kt)("inlineCode",{parentName:"li"},"stress")," to generate load on the cluster, and then verify it is running.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create deployment hog --image=vish/stress\nkubectl get deployments\n")),(0,r.kt)("p",null,"Once the deployment is ready you should see"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"NAME   READY   UP-TO-DATE   AVAILABLE   AGE\nhog    1/1     1            1           11s\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Next, describe the deployment and then view the output as a YAML.  Notice there is not settings limiting the resource use currently, just ",(0,r.kt)("inlineCode",{parentName:"li"},"{}"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl describe deployment hog\nkubectl get deployment hog -o yaml\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"resources")," field in the YAML can be found under ",(0,r.kt)("inlineCode",{parentName:"p"},"spec.template.spec.containers")," and is a setting in each container listed."),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Write the deployment YAML to a file so we can add some resource limits.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get deployment hog -o yaml > hog.yaml\nvim hog.yaml\n")),(0,r.kt)("p",null,"Once the file is open in ",(0,r.kt)("inlineCode",{parentName:"p"},"vim"),", add the following four lines (make sure to indent properly) under the resources section:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'limits:\n  memory: "4Gi"\nrequests:\n  memory: "2500Mi"\n')),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Replace the deployment and verify the change.  You should see the new resources set in YAML output this time matching what you added to the YAML file.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl replace -f hog.yaml\nkubectl get deploymeny hog -o yaml\n")),(0,r.kt)("ol",{start:5},(0,r.kt)("li",{parentName:"ol"},"now go look at the ",(0,r.kt)("inlineCode",{parentName:"li"},"stdio")," of the hog container. Get the pod name, and then look at the container logs.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods\nkubectl logs <hog pod name>\n")),(0,r.kt)("p",null,"The logs should look something like"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'I0405 16:49:23.354678       1 main.go:26] Allocating "0" memory, in "4Ki" chunks, with a 1ms sleep between allocations\nI0405 16:49:23.354726       1 main.go:29] Allocated "0" memory\n')),(0,r.kt)("p",null,"Open a second and third terminal and ssh into the cp node and worker node.  Run ",(0,r.kt)("inlineCode",{parentName:"p"},"top")," in each to view the memory usage. You should not be seeing ",(0,r.kt)("inlineCode",{parentName:"p"},"stress")," as it should not be using enough resources to be seen in the list."),(0,r.kt)("ol",{start:6},(0,r.kt)("li",{parentName:"ol"},"Now let's edit the hog.yaml file again to consume some CPU and memory. Upen the file with ",(0,r.kt)("inlineCode",{parentName:"li"},"vim")," and add the following:")),(0,r.kt)("p",null,"Change the ",(0,r.kt)("inlineCode",{parentName:"p"},"resources")," section to "),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'limits:\n  cpu: "1"\n  memory: "4Gi"\nrequests:\n  cpu: "0.5"\n  memory: "2500Mi"\n')),(0,r.kt)("p",null,"and below the ",(0,r.kt)("inlineCode",{parentName:"p"},"resources")," add a new argument called ",(0,r.kt)("inlineCode",{parentName:"p"},"args")," with the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'args:\n- -cpus\n- "2"\n- -mem-total\n- "950Mi"\n- -mem-alloc-size\n- "100Mi"\n- mem-alloc-sleep\n- "1s"\n')),(0,r.kt)("ol",{start:7},(0,r.kt)("li",{parentName:"ol"},"Now delete the deployment and recreate it.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl delete deployment hog\nkubectl create -f hog.yaml\n")),(0,r.kt)("p",null,"You should now see high usage from ",(0,r.kt)("inlineCode",{parentName:"p"},"stress")," in the ",(0,r.kt)("inlineCode",{parentName:"p"},"top")," table."),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"If the ",(0,r.kt)("inlineCode",{parentName:"p"},"top")," table does not show high usage the workload or the container may have failed."),(0,r.kt)("p",{parentName:"div"},"To check you can get the pods and check the logs"),(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get pods\nkubectl logs <pod name>\n")),(0,r.kt)("p",{parentName:"div"},"The logs and whether the pod is running should give you an idea of what kind of error you have"),(0,r.kt)("ul",{parentName:"div"},(0,r.kt)("li",{parentName:"ul"},"Not running is likely a missing parameter"),(0,r.kt)("li",{parentName:"ul"},"Running is likely an improperly set parameter")))),(0,r.kt)("h3",{id:"lab-43---resource-limits-for-a-namesapce"},"Lab 4.3 - Resource limits for a namesapce"),(0,r.kt)("p",null,"Limits can be set on the deployment level, or at the namespace level. In this lab section we will create a namespace with resource limits and then create another hod deployment to run within that namespace, and it should be limited to less respirce than the previous hog deployment."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"First, we'll create the new namespace, 'low-usage-limit`.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create namespace low-usage-limit\nkubectl get namespace\n")),(0,r.kt)("p",null,"Then create a YAML to define the resource limits for the namespace.  It should be of kind ",(0,r.kt)("inlineCode",{parentName:"p"},"LimitRange"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"vim low-resource-range.yaml\n")),(0,r.kt)("p",null,"and it should contain"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: low-resource-range\nspec:\n  limits:\n  - default:\n      cpu: 1\n      memory: 500Mi\n    defaultRequest:\n      cpu: 0.5\n      memory: 100Mi\n    type: Container\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Now we can create the ",(0,r.kt)("inlineCode",{parentName:"li"},"LimitRange")," object in the new namespace.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n low-usage-limit create -f low-resource-range.yaml\n")),(0,r.kt)("p",null,"And verify it worked"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl get LimitRange --all-namespaces\n")),(0,r.kt)("p",null,"Don't forget the ",(0,r.kt)("inlineCode",{parentName:"p"},"--all-namespaces")," flag, otherwise it will just show the LimitRanges in the default namespace and you will not find anything."),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Now create a new deployment in the new namespace.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n low-usage-limit create deployment limited-hog --image=vish/stress\nkubectl get pods --all-namespaces\n")),(0,r.kt)("p",null,"notice, the first hog deployment is still running."),(0,r.kt)("p",null,"You can view just the pods in the namespace with"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n low-usage-limit get pods\n")),(0,r.kt)("p",null,"Look at the pod details as well."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n low-usage-limit get pod <limited-hog pod name> -o yaml\n")),(0,r.kt)("p",null,"Notice the resources are inherited from the LimitRange."),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Now copy the original ",(0,r.kt)("inlineCode",{parentName:"li"},"hog.yaml")," deployment file and update the ",(0,r.kt)("inlineCode",{parentName:"li"},"namespace")," field with the new one we created and also delete the ",(0,r.kt)("inlineCode",{parentName:"li"},"selflink")," if it exists.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cp hog.yaml hog2.yaml\nvim hog2.yaml\n")),(0,r.kt)("p",null,"And then create a new deployment with the new YAML that was just modified."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl create -f hog2.yaml\nkubectl get deployments --all-namespaces\n")),(0,r.kt)("p",null,"You should see a new deployment named ",(0,r.kt)("inlineCode",{parentName:"p"},"hog")," in the ",(0,r.kt)("inlineCode",{parentName:"p"},"limited-usaged-limit")," namespace."),(0,r.kt)("ol",{start:5},(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"In another window use ",(0,r.kt)("inlineCode",{parentName:"p"},"top")," to view the usage on the nodes.  You should see the new ",(0,r.kt)("inlineCode",{parentName:"p"},"hog")," deployment and the original one at the op of the list using the same amount of resources.  This is because the deployment level limits override any in the namespace.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Delete the deployments to recover system resources."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl -n low-usage-liimit delete deployment hog\nkubectl delete deployment hog\n")),(0,r.kt)("h2",{id:"knowledge-check"},"Knowledge check"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The smallest object or unit that can be worked with in Kubernetes is the ",(0,r.kt)("strong",{parentName:"li"},"Pod")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"One")," IP address can be configured per Pod"),(0,r.kt)("li",{parentName:"ul"},"The main configuration agent on a master server is the ",(0,r.kt)("strong",{parentName:"li"},"kubeapi-server")),(0,r.kt)("li",{parentName:"ul"},"The main agent on a worker node is a ",(0,r.kt)("strong",{parentName:"li"},"kubelet")),(0,r.kt)("li",{parentName:"ul"},"A ",(0,r.kt)("strong",{parentName:"li"},"Service")," connects other resources together and handles Ingres and Egress traffic")))}h.isMDXComponent=!0},8361:function(e,t,a){t.Z=a.p+"assets/images/ch02-architecture-ba5de9e7fccd10f83d78db1eb58862db.png"},4156:function(e,t,a){t.Z=a.p+"assets/images/ch04-container-ip-1509bd6d5c53c8a35f45aa2d4a0d1519.png"},7049:function(e,t,a){t.Z=a.p+"assets/images/ch04-k8s-architecture-review-2fa98b9770107e26700ef65b2a931c67.png"},92:function(e,t,a){t.Z=a.p+"assets/images/ch04-mesos-architecture-ef70cb5319e511349cf256ba849f6c10.jpg"},8913:function(e,t,a){t.Z=a.p+"assets/images/ch04-pod-edbbe7f9bc22ee6daa0463de58e17202.png"},6698:function(e,t,a){t.Z=a.p+"assets/images/ch04-services-2b387831fdba650ff54329f927b99828.png"}}]);