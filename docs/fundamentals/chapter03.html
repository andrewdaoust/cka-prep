<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">Installation and Configuration | CKA Prep</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter03"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Installation and Configuration | CKA Prep"><meta data-react-helmet="true" name="description" content="Course Reading"><meta data-react-helmet="true" property="og:description" content="Course Reading"><link data-react-helmet="true" rel="icon" href="/cka-prep/img/k8s.png"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter03"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter03" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter03" hreflang="x-default"><link rel="stylesheet" href="/cka-prep/assets/css/styles.e68b4784.css">
<link rel="preload" href="/cka-prep/assets/js/runtime~main.ed54229e.js" as="script">
<link rel="preload" href="/cka-prep/assets/js/main.5dd4e585.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cka-prep/"><div class="navbar__logo"><img src="/cka-prep/img/kubernetes-icon.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/cka-prep/img/kubernetes-icon.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">CKA Prep</b></a><a class="navbar__item navbar__link navbar__link--active" href="/cka-prep/docs/intro">Notes</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/andrewdaoust/cka-prep" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cka-prep/docs/intro">Kubernetes Resources</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active hasHref_VCh3" aria-current="page" href="/cka-prep/docs/fundamentals">Kubernetes Fundamentals</a><button aria-label="Toggle the collapsible sidebar category &#x27;Kubernetes Fundamentals&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter01">Course Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter02">Basics of Kubernetes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cka-prep/docs/fundamentals/chapter03">Installation and Configuration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter04">Kubernetes Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter05">APIs and Access</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter06">API Objects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter07">Managing State with Deployments</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter08">Volumes and Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter09">Services</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter10">Helm</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter11">Ingress</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter12">Scheduling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter13">Logging and Troubleshooting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter14">Custom Resource Definitions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter15">Security</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter16">High Availability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter17">Exam Domain Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/follow-up">Follow-up Items</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/cka-prep/docs/kubectl/setup">kubectl</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/cka-prep/docs/hard-way/about">Kubernetes the Hard Way</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/cka-prep/docs/linux">Linux</a><button aria-label="Toggle the collapsible sidebar category &#x27;Linux&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/cka-prep/docs/misc">Miscellaneous Topics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Miscellaneous Topics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Installation and Configuration</h1></header><h2 id="course-reading">Course Reading</h2><h3 id="learning-objectives">Learning objectives</h3><ul><li>Download installation and configuration tools</li><li>Install a Kubernetes master and grow cluster</li><li>Configure network for secure communication</li><li>High-availability deployment considerations</li></ul><h3 id="installation-tools">Installation tools</h3><p>There are many ways to get up an running with Kubernetes.</p><p>If you want to get started without needing to install and configure the cluster yourself, a managed cloud provider solution is a good option.  Google offers <a href="https://cloud.google.com/kubernetes-engine">Google Kubernetes Engine</a> (GKE) and AWS offers <a href="https://aws.amazon.com/eks/">Elastic Kubernetes Service</a> (EKS) which gives users more control of cp nodes.</p><p>Another simple way to get up and running is with <a href="https://minikube.sigs.k8s.io/docs/">Minikube</a> which is a single binary that runs in a VirtualBox VM.  This is a great tool to use as a learning and testing environment even though it is just a single node.</p><p>Canonical has also developed a tool called <a href="https://microk8s.io/docs">MicroK8s</a> which aims to make installation easy.  This is great for running Kubernetes at the edge or on IoT devices. Runs on Ubuntu 16.04 and later.</p><p>This course focuses on using <code>kubeadm</code> which is the suggested community tool by the Kubernetes project for setting up a cluster.  Getting the cluster set up with <code>kubeadm</code> only requires two commands, <code>kubeadm init</code> on the cp node and <code>kubeadm join</code> on any worker nodes or additional cp nodes, and the cluster bootstraps itself.</p><p>To actually use the cluster the <code>kubectl</code> command is used.  This run locally on your machine and communicated with the cluster API endpoint.  <code>kubectl</code> can control all Kubernetes resources to create, manage, and delete.</p><p>There are also other mechanisms to create a Kubernetes cluster, like <a href="https://github.com/kubernetes-sigs/kubespray">kubespray</a> and <a href="https://github.com/kubernetes/kops">kops</a>.</p><h3 id="installing-kubectl">Installing <code>kubectl</code></h3><p>The recommended way to configure and manage your Kubernetes cluster is <code>kubectl</code>.  Most distros have <code>kubectl</code> available in their repositories, or you can download the code from <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl">Github</a> and compile and install from the source code.</p><p>The command stores its configuration in <code>$HOME/.kube/config</code>, which contains information for all the K8s endpoints you might use.  In it are cluster definitions (IP endpoints), credentials, and contexts.  The contexts is a combination of cluster and user credentials, which can be passed in the command line or the context can be switched with </p><pre><code class="language-bash">kubectl config use-context foobar
</code></pre><p>which is helpful for switching between local environments to a cluster in the cloud.</p><h3 id="using-google-kubernetes-engine-gke">Using Google Kubernetes Engine (GKE)</h3><p>Requirements:</p><ul><li>A Google Cloud account</li><li>Payment method for any services used</li><li><code>gcloud</code> command line client.</li></ul><p>Instructions for installing <code>gcloud</code> can be found <a href="https://cloud.google.com/sdk/docs/install#linux">here</a>.</p><p>The GKE quick start guide can be found <a href="https://cloud.google.com/kubernetes-engine/docs/quickstart">here</a>.</p><p>Then to create your first cluster in GKE:</p><pre><code class="language-bash">gcloud container clusters create linuxfoundation

gcloud container clusters list

kubectl get nodes
</code></pre><p>The first command creates the cluster with the name <em>linuxfoundation</em>.  The next command lists the cluster. The final command lists the nodes of the cluster. Installing <code>gcloud</code> automatically installs <code>kubectl</code> as well.</p><p>Once done with the cluster, <strong>delete it or you will be charged</strong>.</p><pre><code class="language-bash">gcloud container clusters delete linuxfoundation
</code></pre><h3 id="using-minikube">Using minikube</h3><p>Minikube is a project under the <a href="https://github.com/kubernetes/minikube">Kubernetes organization on Github</a>.</p><p>To download and install the latest version of minikube run</p><pre><code class="language-bash">curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x minikube

sudo mv minikube /usr/local/bin
</code></pre><p>Once minikube is installed, the Kubernetes can be started with</p><pre><code class="language-bash">minikube start

kubectl get nodes
</code></pre><p>The first command will start a VirtualBox VM running a single node Kubernetes deployment and the Docker engine.  Internally, minikube is running a single Golang binary, <code>localkube</code>, which runs all the components on Kubernetes together, which makes minikube much simpler than a full Kubernetes deployment.</p><h3 id="installing-with-kubeadm">Installing with <code>kubeadm</code></h3><p>Currently, the most straightforward method of building a full Kubernetes cluster is <code>kubeadm</code> which was first introduced in v1.4.0 and moved from beta to stable with added functionality for high availability in v1.15.0.</p><p>Official documentation for setting up a cluster with <code>kubeadm</code> can be found <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">here</a>.</p><p>To setup a cluster:</p><ul><li>Run <code>kubeadm init</code> on head node</li><li>Create network for IP-per-Pod criteria</li><li>Run <code>kubeadm join</code> on workers or secondary cp nodes.</li></ul><p>Joining other nodes to the cluster will require at least a token and a SHA256 hash, which is returned by the <code>kubeadm init</code>. <code>kubectl</code> can also be used to create the network using a resource manifest.</p><p>An example, using the Weave network:</p><pre><code class="language-bash">kubectl create -f https://git.io/weave-kube
</code></pre><p>Once the steps are complete and the workers and secondary cp nodes have joined, there will be an operational multi-node Kubernetes cluster and you can interact with it using <code>kubectl</code>.</p><h3 id="kubeadm-upgrade"><code>kubeadm-upgrade</code></h3><p>Building the cluster with <code>kubeadm</code> gives the option to upgrade using <code>kubeadm upgrade</code>.  Most users will try to stay on the same version for as long as possible, but this tool allows a path for regular upgrades for security.</p><p>Some of the command for <code>kubeadm upgrade</code>:</p><ul><li><code>plan</code> checks the installed version against the newest version to verify upgradeability</li><li><code>apply</code> upgrades the first cp node to the version specified</li><li><code>diff</code> shows the differences applied in an upgrade. It is similar to <code>apply --dry-run</code>.</li><li><code>node</code> lets local kubelet configuration to be updated on the worker nodes, or secondary cp nodes. Also calls a <code>phase</code> command to step through upgrading.</li></ul><p>In general, the process for upgrading is:</p><ul><li>Update software</li><li>Check version</li><li>Drain cp</li><li>View planned upgrade</li><li>Apply upgrade</li><li>Uncordon the cp and allow pods to be scheduled</li></ul><p>More in-depth documentation about the upgrade process can be found in the <a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/">official documentation here</a>.</p><h3 id="installing-a-pod-network">Installing a Pod network</h3><p>Prior to initialization of a cluster, a network needs to be considered and IP conflicts avoided.  There are many options for Pod networking. Many projects mention Container Network Interface (or CNI, another CNCF project) as a way to handle deployments and cleaning up network resources.</p><h4 id="calico">Calico</h4><p>A flat layer 3 network that communicates without IP encapsulation. It is used in production with many orchestration tools.  It has a simple and flexible networking model and scales well to large environments.  Canal is another option, that is part of the same project, which can integrate with Flannel.  Calico also allows implementing network policies.  </p><p><a href="https://www.tigera.io/project-calico/">Project webpage</a></p><h4 id="flannel">Flannel</h4><p>A layer 3 IPv4 network between cluster nodes. It has a long history with Kubernetes as it as developed by CoreOS.  Flannel focuses on traffic between hosts, not the local container configuration for networking. <code>flanneld</code> agents sit on each node to allocate subnet leases to the host.  It can be configured after deployment but it is easier before Pods are added.</p><p><a href="https://github.com/flannel-io/flannel">Project Github</a></p><h4 id="kube-router">Kube-Router</h4><p>A single binary that &quot;<em>does it all</em>&quot;.  It is in alpha but promises distributed load balancer, firewall, and router purpose built for K8s.</p><p><a href="https://www.kube-router.io/">Project webpage</a></p><h4 id="romana">Romana</h4><p>This project is aimed towards automating network and security in cloud native applications.  Romana is aimed at large clusters, IPAM-aware topology and integrates with kops clusters.</p><p><a href="https://github.com/romana/romana">Project Github</a></p><h4 id="weave-net">Weave Net</h4><p>Usually used as an add-on for CNI enabled clusters.</p><p><a href="https://www.weave.works/oss/net/">Project webpage</a></p><h3 id="more-installation-tools">More installation tools</h3><p>Kubernetes is like any other application you would install on a server so the the usual configuration management tools (Terraform, Ansible, Chef, Puppet, etc.) can be used for installation.</p><p>The best way to learn about installing Kubernetes manually is with Kelsey Hightower&#x27;s <a href="../hard-way/about">Kubernetes the Hard Way</a>.</p><h4 id="kubespray">kubespray</h4><p>kubespray is a Kubernetes incubator project. It is an advanced ansible playbook that can set up a cluster on different OSes with different network providers.  It was formerly known as kargo.</p><p><a href="https://github.com/kubernetes-sigs/kubespray">Project Github</a></p><h4 id="kops">kops</h4><p>kops (short for Kubernetes Operations) allows for single command line creation of K8s cluster on AWS.  Creation on GKE is in beta and VMWare is in alpha.</p><p><a href="https://github.com/kubernetes/kops">Project Github</a></p><h4 id="kube-aws">kube-aws</h4><p>A command line tool for creating Kubernetes clusters on AWS using CloudFormation.  This tool has been retired and has reached end of life.</p><p><a href="https://github.com/kubernetes-retired/kube-aws">Project Github</a></p><h4 id="kubicorn">kubicorn</h4><p>kubicorn leverages kubeadm to build clusters.  It has no DNS dependency, runs on multiple OSes, and uses snapshots to capture clusters and move them.</p><p><a href="http://kubicorn.io/">Project Github</a></p><h3 id="installation-considerations">Installation considerations</h3><p>Before installing a full cluster, it is good to experiment with a single node cluster, like the one minikube provides.</p><p>Once ready to deploy a cluster of servers there are a some decision points</p><ul><li>What provider? Public or private cloud? Virtual or physical servers?</li><li>What operating system? Kubernetes runs on most Linux distros.</li><li>What networking solution? Is an overlay needed?</li><li>High availability for the head nodes?</li></ul><p>To choose the best option, the Kubernetes <a href="https://kubernetes.io/docs/setup/"><em>Getting Started</em></a> docs are a good resource.</p><p>In most cases the Kubernetes components will run as <code>systemd</code> unit files as that has become the dominant init system for Linux OSes.  They could also be run by a kubelet on the head node (kubeadm).</p><h3 id="main-deployment-configurations">Main deployment configurations</h3><p>There are four main deployment configurations.</p><ul><li><strong>Single node</strong> - all components run on the same server, Good for testing and development, not well suited for production.</li><li><strong>Single head node, multiple workers</strong> - Typically has an etcd instance running on the head node with the API, scheduler, and controller-manager.</li><li><strong>Multiple head nodes with HA, multiple workers</strong> - This type of configuration adds more durability to the cluster. The API server is fronted by a load balancer, scheduler and controller-manager elect a leader (configured by flags). etcd can still run as a single node.</li><li><strong>HA etcd, HA head nodes, multiple workers</strong> - This is the most advanced and robust Kubernetes setup.  etcd would also run as a true cluster on nodes separate from th head nodes.</li></ul><p>A tool called Kubernetes Federation also offers high availability.  It joins multiple clusters together with a common cp to let resources move between clusters administratively or due to failure.  It has some issues but there is hope <a href="https://github.com/kubernetes-sigs/kubefed">v2</a> will be a better product.</p><h3 id="systemd-unit-file-for-kubernetes"><code>systemd</code> unit file for Kubernetes</h3><p>In any of the mentioned configurations, some components will run as standard system daemons. Here is an example of a (by no means perfect) <code>systemd</code> unit file for the controller-manager:</p><pre><code class="language-yaml">- name: kube-controller-manager.service
    command: start 
    content: |
      [Unit]
      Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/...
      Requires=kube-apiserver.service
      After=kube-apiserver.service
      [Service]
      ExecStartPre=/usr/bin/curl -L -o /opt/bin/kube-controller-manager -z /opt/bin/kube-controller-manager https://storage.googleapis.com...
      ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-controller-manager
      ExecStart=/opt/bin/kube-controller-manager \
        --service-account-private-key-file=/opt/bin/kube-serviceaccount.key \
        --root-ca-file=/var/run/kubernetes/apiserver.crt \
        --cp=127.0.0.1:8080 \
...
</code></pre><p>Familiarity with the configuration of each components and the options available come with more practice.  Expect the option to change as Kubernetes continues to rapidly develop.</p><p>An example, the API serve is a highly configurable component. Here&#x27;s the <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">documentation for configuring</a> it.</p><h3 id="using-hyperkube">Using Hyperkube</h3><p>Instead of system deamons, the API server, scheduler, and controller-manager can be run as containers.  This is how <code>kubeadm</code> runs them. Similar to minikube, hyperkube runs as an all in one binary <a href="https://console.cloud.google.com/gcr/images/google-containers/GLOBAL/hyperkube">which Google hosts</a> as a container (this may require adding a new repository so Docker can find and download the image).</p><p>Using hyperkube runs a kubelet as a system daemon and then reads manifests for instructions on how to run the other components.  Running hyperkube is also a good way to begin learning the different configuration flags of the components that form the cp.  You can get more information on these flags by downloading the image and running the help commands:</p><pre><code class="language-bash">docker run --rm gcr.io/google_containers/hyperkube:v1.16.7 /hyperkube kube-apiserver --help

docker run --rm gcr.io/google_containers/hyperkube:v1.16.7 /hyperkube kube-scheduler --help

docker run --rm gcr.io/google_containers/hyperkube:v1.16.7 /hyperkube kube-controller-manager --help
</code></pre><h3 id="compiling-from-source">Compiling from source</h3><p>Apart from these useful tool, Kubernetes can also be compiled from source by cloning the repository and building the binaries.  Building can be done natively with Golang or via Docker containers.</p><p>To build via Golang, first <a href="https://go.dev/doc/install">install it</a>. Then clone the <a href="https://github.com/kubernetes/kubernetes"><code>kubernetes</code> repo</a> and run the <code>make</code> command:</p><pre><code class="language-bash">cd $GOPATH
git clone https://github.com/kubernetes/kubernetes
cd kubernetes
make
</code></pre><p>For building with Docker, instead of <code>make</code> run <code>make quick-release</code>.</p><p>The built binaries will be in the <code>__output/bin</code> directory.</p><h2 id="lab-exercises">Lab Exercises</h2><h3 id="lab-31---install-kubernetes">Lab 3.1 - Install Kubernetes</h3><ol><li>SSH into master node</li></ol><pre><code class="language-bash">ssh -i &lt;PEM key name&gt; &lt;user&gt;@&lt;IP address&gt;
</code></pre><ol start="2"><li><code>wget</code> the course materials.</li></ol><div class="admonition admonition-important alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>important</h5></div><div class="admonition-content"><p>Check the <a href="https://training.linuxfoundation.org/cm/LFS258/">course material page</a> before running to make sure you are downloading the latest tarball</p></div></div><pre><code class="language-bash">wget https://training.linuxfoundation.org/cm/LFS258/LFS258_V2021-09-20_SOLUTIONS.tar.xz \
    --user=LFtraining --password=Penguin2014

tar -xvf LFS258_V2021-09-20_SOLUTIONS.tar.xz
</code></pre><ol start="3"><li>Become root and update/upgrade the system.</li></ol><pre><code class="language-bash">sudo -i

apt-get update &amp;&amp; apt-get upgrade -y
</code></pre><p>The <code>-y</code> flag will accept all prompt to make the upgrade go faster so you are not prompted for every package upgrade.</p><ol start="4"><li>Install an editor, <code>nano</code>, <code>vim</code>, and <code>emacs</code> all work well.  The labs are designed to use <code>vim</code></li></ol><pre><code class="language-bash">apt-get install -y vim
</code></pre><ol start="5"><li>Install container runtime.  The course suggests Docker, as that is the default runtime when building with <code>kubeadm</code> on Ubuntu at the moment.  For an added challenge (maybe try these labs again with this option?) you could use cri-o, but at the moment that takes multiple steps.</li></ol><pre><code class="language-bash">apt-get install -y docker.io
</code></pre><ol start="6"><li>Add a new repo for Kubernetes. Create the file and add an entry for the main repo for the distro we are using (Ubuntu in my case).  Even though we are using Ubuntu 18.04, we&#x27;ll use the <code>kubernetes-xenial</code> repo. Also include the keyword <code>main</code>  Note there are four sections to the entry.</li></ol><p>Creating the file:</p><pre><code class="language-bash">vim /etc/apt/sources.list.d/kubernetes.list
</code></pre><p>The entry should look like:</p><pre><code class="language-bash">deb http://apt.kubernetes.io/ kubernetes-xenial main
</code></pre><ol start="7"><li>Add a GPG key for the packages</li></ol><pre><code class="language-bash">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
</code></pre><ol start="8"><li>Update the system again, with the new repo declared to download the latest repo information.</li></ol><pre><code class="language-bash">apt-get update
</code></pre><ol start="9"><li>Install the software.  New versions release regularly but there are often bugs. To use the latest, omit the <code>=&lt;version&gt;</code> from the following commands. Because of this we will install the most recent stable versions. In a later lab, the cluster will be upgraded to a newer version.</li></ol><pre><code class="language-bash">apt-get install -y kubeadm=1.21.1-00 kubelet=1.21.1-00 kubectl=1.21.1-00

apt-mark hold kubelet kubeadm kubectl
</code></pre><ol start="10"><li>Decide on a pod network.  As discussed previously, this should take into account the anticipated demands on the cluster.  There can only be one pod network per cluster, although there is a project, CNI-genie that is trying to change that.</li></ol><p>The network needs to allow container-to-container, pod-to-pod, pod-to-service, and external-to-service communication. Docker uses host-private networking (<code>docker0</code> virtual bridge and <code>veth</code> interfaces) which requires being on the host to communicate.</p><p>For this we will use Calico as the network plugin to use <em>Network Policies</em> later in the course. Calico does not deploy using the CNI by default at the moment. Once downloaded, we need to look for the expected IPv4 range the containers will use in the configuration file.</p><pre><code class="language-bash">wget https://docs.projectcalico.org/manifests/calico.yaml
</code></pre><ol start="11"><li>With the manifest downloaded, look through it for the IPv4 pool assigned to the containers (the <code>less</code> or <code>cat</code> commands are good for this).  Also take a moment while paging through to look at some of the other settings. The <code>CALICO_IPV4POOL_CIDR</code> must match the pool given to <code>kubeadm init</code>.</li></ol><pre><code class="language-bash">cat calico.yaml
</code></pre><p><img alt="calico ipv4 pool" src="/cka-prep/assets/images/ch03-lab-calico-ip-pool-b8ebdc30af337f6f3c6c4fcdeabd09a1.png" width="1286" height="194"></p><ol start="12"><li>Find the IP address for the primary interface of the cp server.  There are two ways to do this:</li></ol><pre><code class="language-bash">hostname -i
</code></pre><p>or </p><pre><code class="language-bash">ip addr show
</code></pre><ol start="13"><li>Add a local DNS alias for the cp server by editing the /etc/hosts file and adding an entry for the IP we found in the last step with the alias <code>k8scp</code>.</li></ol><pre><code class="language-bash">vim /etc/hosts
</code></pre><ol start="14"><li>Create a configuration file for the cluster. For Docker, we need to configure the cp endpoint, software version, and podSubnet values.  For cri-o there are many more things needed in the configuration.</li></ol><pre><code class="language-bash">vim kubeadm-config.yaml
</code></pre><p>In the config file should be this:</p><pre><code class="language-yaml">apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: 1.21.1           #&lt;-- Use the word stable for newest version
controlPlaneEndpoint: &quot;k8scp:6443&quot;  #&lt;-- Use the node alias not the IP
networking:
  podSubnet: 192.168.0.0/16         #&lt;-- Match the IP range from the Calico config file
</code></pre><ol start="15"><li>Initialize the cp. This output is likely to change as the software continues to mature. It will also give you a token to use to join worker nodes later, this can be retrieved again using <code>kubeadm token list</code>.  You will also be asked to configure a pod network, and the Calico configuration file will be passed for this.</li></ol><pre><code class="language-bash">kubeadm init --config=kubeadm-config.yaml --upload-certs | tee kubeadm-init.out
</code></pre><p>Once the cp is initialize, there are commands to run not as root. To exit as the root user run</p><pre><code class="language-bash">exit
</code></pre><p>Then run the following commands:</p><pre><code class="language-bash">mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

less .kube/config
</code></pre><ol start="17"><li>Apply the network plugin</li></ol><pre><code class="language-bash">sudo cp /root/calico.yaml .

kubectl apply -f calico.yaml
</code></pre><ol start="18"><li>We&#x27;ll next install the autocompletion to make working with <code>kubectl</code> a little easier.</li></ol><pre><code class="language-bash">sudo apt-get install bash-completion -y
</code></pre><p>Then restart the terminal session and do the following:</p><pre><code class="language-bash">source &lt;(kubectl completion bash)

echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; $HOME/.bashrc
</code></pre><ol start="19"><li><p>Test the autocomplete installation worked using tab as you type to autocomplete the command.</p></li><li><p>View the config values that we could have used in the <code>kubeadm-config.yaml</code>.</p></li></ol><pre><code class="language-bash">sudo kubeadm config print init-defaults
</code></pre><h3 id="lab-32---grow-the-cluster">Lab 3.2 - Grow the cluster</h3><ol><li><p>SSH into the worker node and use the same process as the cp node to get all the software installed on the worker node. This is steps 1 and 3-9 in the <a href="#lab-31---install-kubernetes">previous section</a>.</p></li><li><p>Find your IP address for the <strong>cp node</strong>.  Reminder that you can do this again with <code>hostname -i</code>.</p></li><li><p>Next we want to join the cp node and the worker.  Remember that the join command is printed in the console but this command only works for 2 hours until it expires, so in the future we will need to generate our own.  To do this:</p></li></ol><pre><code class="language-bash">sudo kubeadm token create
</code></pre><p>And then to list the token run:</p><pre><code class="language-bash">sudo kubeadm token list
</code></pre><ol start="4"><li>Create a Discovery Token CA Cert Hash on the <strong>cp</strong> to make sure there is a secure connection between it and the worker node.</li></ol><pre><code class="language-bash">openssl x509 -pubkey \
    -in /etc/kubernetes/pki/ca.crt | openssl rsa \
    -pubin -outform der 2&gt;/dev/null | openssl dgst \
    -sha256 -hex | sed&#x27;s/Ë†.* //&#x27;
</code></pre><ol start="5"><li>On the <strong>worker</strong> node add a hostname alias for the <strong>cp</strong> name like we did originally on the cp node in the previous lab, with the alias <code>k8scp</code>.</li></ol><pre><code class="language-bash">vim /etc/hosts
</code></pre><ol start="6"><li>Next we can join the <strong>worker</strong>/<strong>second</strong> node to the <strong>cp</strong>.  We will use the token and the hash (a <code>sha256</code>) to join them.  The <code>kube init</code> would have an example of this to use if within 2 hours of running the command. Otherwise, we would build the command from the token and hash we just created. We&#x27;ll also use the hostname alias we setup and port <code>6443</code>.</li></ol><pre><code class="language-bash">kubeadm join \
    --token &lt;token&gt; \
    k8scp:6443 \
    --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>You can check this worked by running </p><pre><code class="language-bash">kubectl get nodes
</code></pre><p>on the <strong>cp</strong> node.  </p><ol start="7"><li>Now exit root on the <strong>worker</strong> node and try to run kubectl to get the nodes. It should fail because there is no local configuration to access the cluster.</li></ol><pre><code class="language-bash">exit
kubectl get nodes
ls -l .kube
</code></pre><p>The second like should fail due to the lack of configuration and the 3rd should fail due to the file not existing.</p><h3 id="lab-33---finish-cluster-setup">Lab 3.3 - Finish cluster setup</h3><ol><li>View available nodes of the cluster.  On the <strong>cp</strong> node run</li></ol><pre><code class="language-bash">kubectl get nodes
</code></pre><ol start="2"><li>Look at the details on the <strong>cp</strong> node. Notice that <code>Taints</code>. The cp does not run non-infrastructure pods by default for security and resource contention.</li></ol><pre><code class="language-bash">kubectl describe node k8scp
</code></pre><ol start="3"><li>Enable non-infrastructure pods to run. For training we allow usage of the node but this can be skipped when setting up for a production environment.</li></ol><pre><code class="language-bash">kubectl describe node | grep -i taint

kubectl taint nodes --all node-role.kubernetes.io/master-
</code></pre><ol start="4"><li>Determine if the DNS and Calico pods are ready for use.</li></ol><pre><code class="language-bash">kubectl get pods --all-namespaces
</code></pre><p>If the CoreDNS pods seem to be getting stuck you may need to delete them to force them to be recreated.</p><pre><code class="language-bash">kubectl -n kube-system delete coredns-&lt;instance&gt; coredns-&lt;instance&gt;
</code></pre><ol start="5"><li>Once this finishes, run </li></ol><pre><code class="language-bash">ip a
</code></pre><p>and you should see a new tunnel interface, <code>tunl0</code>, and more new interfaces as other pods are deployed.</p><h3 id="lab-34---deploy-a-simple-application">Lab 3.4 - Deploy a simple application</h3><ol><li>Create a new <code>deployment</code>, which deploys a new container running an application and verify it is running.</li></ol><pre><code class="language-bash">kubectl create deployment nginx --image=nginx
kubectl get deployments
</code></pre><ol start="2"><li>View the details of the deployment</li></ol><pre><code class="language-bash">kubectl describe deployment nginx
</code></pre><ol start="3"><li>View the basic steps the cluster made to create the deployment</li></ol><pre><code class="language-bash">kubectl get events
</code></pre><ol start="4"><li>Get the description of the deployment in YAML format and notice, about halfway down in the output is the current status of the deployment.</li></ol><pre><code class="language-bash">kubectl get deployment nginx -o yaml
</code></pre><ol start="5"><li>Run the command but pipe the output to a file. Then edit the file and remove <code>creationTimestamp</code>,<code>resourceVersion</code>, and <code>uid</code> sections. Also remove everything from <code>status</code> down.</li></ol><pre><code class="language-bash">kubectl get deployment nginx -o yaml &gt; first.yaml

vim first.yaml
</code></pre><ol start="6"><li>Delete the existing deployment</li></ol><pre><code class="language-bash">kubectl delete deployment nginx
</code></pre><ol start="7"><li>Recreate the deployment, this time with our edited YAML file.</li></ol><pre><code class="language-bash">kubectl create -f first.yaml
</code></pre><ol start="8"><li>Get the output of this deployment and compare it to the first.</li></ol><pre><code class="language-bash">kubectl get deployment nginx -o yaml &gt; second.yaml

diff first.yaml second.yaml
</code></pre><ol start="9"><li>Now we will learn some ways to get useful YAML and JSON output.  The first is by &quot;creating&quot; a deployment but use the <code>--dry-run</code> flag to just see the deployment spec. It should look very similar to the ones modified in previous steps.  Also verify that no deployment was actually created by getting the deployment and verifying only the original <code>nginx</code> deployment is there.</li></ol><pre><code class="language-bash">kubectl create deployment two --image=nginx --dry-run=client -o yaml
kubectl get deployment
</code></pre><p>We can also get the YAML for an existing deployment as seen in previous steps.</p><pre><code class="language-bash">kubectl get deployments nginx -o yaml
</code></pre><p>We can also output to JSON.</p><pre><code class="language-bash">kubectl get deployments nginx -o json
</code></pre><ol start="10"><li>Now back to our <code>nginx</code> deployment.  To be able to talk to the web server from external points of the cluster we need to create a <code>service</code>.  First look at the help page for the <code>expose</code> command. Notice some of the examples in the help field.</li></ol><pre><code class="language-bash">kubectl expose -h
</code></pre><p>Try to gain access to the server but notice it will fail since a port was not given.</p><pre><code class="language-bash">kubectl expose deployment/nginx
</code></pre><ol start="11"><li>Now update the deployment container spec in the YAML file with the port information.</li></ol><pre><code class="language-bash">vim first.yaml
</code></pre><p>and add the following fields under the <code>spec.template.spec.containers</code> section of the file.</p><pre><code class="language-yaml">ports:
- containerPort: 80
  protocol: TCP
</code></pre><p>There are a few subcommands that will update the configuration. <code>apply</code>, <code>edit</code>, and <code>path</code> all do it non-disruptively.</p><p><code>apply</code> does a 3-way diff on the previous, current, and supplied input to determine what changes to make.  Fields that are not mentioned will not be touched.</p><p><code>edit</code> gets the current configuration, opens an editor, and then runs an <code>apply</code> on the made changes.</p><p><code>patch</code> can be used to update API objects in place.</p><p>For changes that cannot be made once te object is initialized, <code>replace</code> can be used which will destroy the object and recreate it.  For the <code>nginx</code> deployment we must do this.</p><pre><code class="language-bash">kubectl replace -f first.yaml
</code></pre><p>Then check to make sure the deployment and pod status show that they are ready.</p><pre><code class="language-bash">kubectl get deploy,pod
</code></pre><ol start="12"><li>Now try to expose the web server again.</li></ol><pre><code class="language-bash">kubectl expose deployment/nginx
</code></pre><p>Then check the service and endpoint information.  Take note of the <code>ClusterIP</code> (provided by Calico) in the service information and the <code>Endpoint</code> in the endpoint information to use for later.</p><pre><code class="language-bash">kubectl get svc nginx

kubectl get ep nginx
</code></pre><p>10.96.14.131
192.168.157.133:80</p><ol start="13"><li>Determine which node the container is running on.  Log into that node and run a <code>tcpdump</code> (this may need to be installed) to see the traffic on <code>tunl0</code>.  While the <code>tcpdump</code> is still running use <code>curl</code> to send an HTTP request.</li></ol><p>On the <strong>cp node</strong>:</p><pre><code class="language-bash">kubectl describe pod nginx-&lt;specific deployment&gt; | grep Node:
</code></pre><p>On the node that is running the pod (in my case thw worker node):</p><pre><code class="language-bash">sudo tcpdump -i tunl0
</code></pre><p>Now <code>curl</code> the <code>ClusterIP</code> on port <code>80</code> and also try to <code>curl</code> the <code>Endpoint</code>. You should get the same response.</p><pre><code class="language-bash">curl &lt;ClusterIP&gt;:80
curl &lt;Endpoint&gt;
</code></pre><ol start="14"><li>Now scale the deployment to three web servers.</li></ol><pre><code class="language-bash">kubectl get deployment

kubectl scale deployment nginx --replicas=3

kubectl get deployment nginx
</code></pre><p><img alt="scaled deployment" src="/cka-prep/assets/images/ch03-lab-scale-deployment-63b95ad4e3566a7760c2fe2da51a5bfe.png" width="650" height="154"></p><ol start="14"><li>Now look at the endpoints again. There should now be three.</li></ol><pre><code class="language-bash">kubectl get ep nginx
</code></pre><p>Now find the oldest deployment running and delete it so that it is recreated.</p><pre><code class="language-bash">kubectl get pod -o wide
kubectl delete pod nginx-&lt;specific deployment&gt; 
</code></pre><p>Then confirm the new pod is running. You should see one that is newer than the other two.</p><pre><code class="language-bash">kubectl get po
</code></pre><p>If you view the endpoints again you will notice the original IP is no longer in use. Try to <code>curl</code> the <code>ClusterIP</code> and any of the <code>Endpoints</code> again. You should still have access to the cluster.  Access is only available within the cluster though.  Once doe you can stop the <code>tcpdump</code> with <code>ctrl-C</code>.</p><h3 id="lab-35---access-from-outside-the-cluster">Lab 3.5 - Access from outside the cluster</h3><p>Access to the cluster from external sources can be configured using Services with a DNS-addon or environment variables.  We&#x27;ll use environment variables.</p><ol><li>Gt this list of pods and then <code>exec</code> into on to print the environment variables with <code>printenv</code>.</li></ol><pre><code class="language-bash">kubectl get po

kubectl exec nginx-&lt;specific deployment&gt; -- printenv | grep KUBERNETES
</code></pre><ol start="2"><li>Find the existing service for <code>nginx</code> and delete it.</li></ol><pre><code class="language-bash">kubectl get svc

kubectl delete svc nginx
</code></pre><ol start="3"><li>Now create the service again, but this time as type <code>LoadBalancer</code>.</li></ol><pre><code class="language-bash">kubectl expose deployment nginx --type=LoadBalancer

kubectl get svc
</code></pre><p>Note the <code>EXTERNAL_IP</code> will be pending until a provider responds with a load balancer.</p><ol start="4"><li><p>Now on your local machine, in the browser, type the public IP of the node and the port given for the service in the previous step.  You should get the nginx welcome page.</p></li><li><p>Now scale down the replicas to zero and confirm they are all down.</p></li></ol><pre><code class="language-bash">kubectl scale deployment nginx --replicas=0

kubectl get po
</code></pre><p>Access to the web server should fail now. Scale back up to two replicas and try again, the web server should be working.</p><ol start="6"><li>Now delete the deployment to recover the system resources.  Note that the service and endpoints need to also be deleted.</li></ol><pre><code class="language-bash">kubectl delete deployments nginx

kubectl delete ep nginx

kubectl delete svc nginx
</code></pre><h2 id="knowledge-check">Knowledge check</h2><ul><li><code>kubeadm</code> is used to <strong>create a cluster and add nodes</strong></li><li>The main binary for working with object of a Kubernetes cluster is <strong><code>kubectl</code></strong></li><li>There can be <strong>1</strong> pod network per cluster</li><li>The <code>~/.kube/config</code> file contains <strong>endpoints</strong>, <strong>SSL keys</strong>, and <strong>contexts</strong>.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fundamentals/chapter03.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/cka-prep/docs/fundamentals/chapter02"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Basics of Kubernetes</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/cka-prep/docs/fundamentals/chapter04"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Kubernetes Architecture</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#course-reading" class="table-of-contents__link toc-highlight">Course Reading</a><ul><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning objectives</a></li><li><a href="#installation-tools" class="table-of-contents__link toc-highlight">Installation tools</a></li><li><a href="#installing-kubectl" class="table-of-contents__link toc-highlight">Installing <code>kubectl</code></a></li><li><a href="#using-google-kubernetes-engine-gke" class="table-of-contents__link toc-highlight">Using Google Kubernetes Engine (GKE)</a></li><li><a href="#using-minikube" class="table-of-contents__link toc-highlight">Using minikube</a></li><li><a href="#installing-with-kubeadm" class="table-of-contents__link toc-highlight">Installing with <code>kubeadm</code></a></li><li><a href="#kubeadm-upgrade" class="table-of-contents__link toc-highlight"><code>kubeadm-upgrade</code></a></li><li><a href="#installing-a-pod-network" class="table-of-contents__link toc-highlight">Installing a Pod network</a></li><li><a href="#more-installation-tools" class="table-of-contents__link toc-highlight">More installation tools</a></li><li><a href="#installation-considerations" class="table-of-contents__link toc-highlight">Installation considerations</a></li><li><a href="#main-deployment-configurations" class="table-of-contents__link toc-highlight">Main deployment configurations</a></li><li><a href="#systemd-unit-file-for-kubernetes" class="table-of-contents__link toc-highlight"><code>systemd</code> unit file for Kubernetes</a></li><li><a href="#using-hyperkube" class="table-of-contents__link toc-highlight">Using Hyperkube</a></li><li><a href="#compiling-from-source" class="table-of-contents__link toc-highlight">Compiling from source</a></li></ul></li><li><a href="#lab-exercises" class="table-of-contents__link toc-highlight">Lab Exercises</a><ul><li><a href="#lab-31---install-kubernetes" class="table-of-contents__link toc-highlight">Lab 3.1 - Install Kubernetes</a></li><li><a href="#lab-32---grow-the-cluster" class="table-of-contents__link toc-highlight">Lab 3.2 - Grow the cluster</a></li><li><a href="#lab-33---finish-cluster-setup" class="table-of-contents__link toc-highlight">Lab 3.3 - Finish cluster setup</a></li><li><a href="#lab-34---deploy-a-simple-application" class="table-of-contents__link toc-highlight">Lab 3.4 - Deploy a simple application</a></li><li><a href="#lab-35---access-from-outside-the-cluster" class="table-of-contents__link toc-highlight">Lab 3.5 - Access from outside the cluster</a></li></ul></li><li><a href="#knowledge-check" class="table-of-contents__link toc-highlight">Knowledge check</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cka-prep/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/andrewdaoust/cka-prep" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Andrew D'Aoust. Built with Docusaurus.</div></div></div></footer></div>
<script src="/cka-prep/assets/js/runtime~main.ed54229e.js"></script>
<script src="/cka-prep/assets/js/main.5dd4e585.js"></script>
</body>
</html>