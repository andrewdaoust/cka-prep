<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<title data-react-helmet="true">Scheduling | CKA Prep</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter12"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Scheduling | CKA Prep"><meta data-react-helmet="true" name="description" content="Course Reading"><meta data-react-helmet="true" property="og:description" content="Course Reading"><link data-react-helmet="true" rel="icon" href="/cka-prep/img/k8s.png"><link data-react-helmet="true" rel="canonical" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter12"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter12" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://your-docusaurus-test-site.com/cka-prep/docs/fundamentals/chapter12" hreflang="x-default"><link rel="stylesheet" href="/cka-prep/assets/css/styles.e68b4784.css">
<link rel="preload" href="/cka-prep/assets/js/runtime~main.c69fe18d.js" as="script">
<link rel="preload" href="/cka-prep/assets/js/main.32f4f9d5.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/cka-prep/"><div class="navbar__logo"><img src="/cka-prep/img/kubernetes-icon.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/cka-prep/img/kubernetes-icon.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">CKA Prep</b></a><a class="navbar__item navbar__link navbar__link--active" href="/cka-prep/docs/intro">Notes</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/andrewdaoust/cka-prep" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/cka-prep/docs/intro">Kubernetes Resources</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active hasHref_VCh3" aria-current="page" href="/cka-prep/docs/fundamentals">Kubernetes Fundamentals</a><button aria-label="Toggle the collapsible sidebar category &#x27;Kubernetes Fundamentals&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter01">Course Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter02">Basics of Kubernetes</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter03">Installation and Configuration</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter04">Kubernetes Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter05">APIs and Access</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter06">API Objects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter07">Managing State with Deployments</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter08">Volumes and Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter09">Services</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter10">Helm</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter11">Ingress</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/cka-prep/docs/fundamentals/chapter12">Scheduling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter13">Logging and Troubleshooting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter14">Custom Resource Definitions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter15">Security</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter16">High Availability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/chapter17">Exam Domain Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/cka-prep/docs/fundamentals/follow-up">Follow-up Items</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/cka-prep/docs/kubectl/setup">kubectl</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/cka-prep/docs/hard-way/about">Kubernetes the Hard Way</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/cka-prep/docs/linux">Linux</a><button aria-label="Toggle the collapsible sidebar category &#x27;Linux&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link hasHref_VCh3" href="/cka-prep/docs/misc">Miscellaneous Topics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Miscellaneous Topics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Scheduling</h1></header><h2 id="course-reading">Course Reading</h2><h3 id="learning-objectives">Learning objectives</h3><ul><li>Learn how the kube-scheduler schedules pods</li><li>Use labels to manage the scheduling of pods</li><li>Configure taints and tolerations</li><li>Use <code>podAffinity</code> and <code>podAntiAffinity</code></li><li>Understand how to run multiple schedulers</li></ul><h3 id="kube-scheduler">kube-scheduler</h3><p>Scheduling becomes more important as a deployment of Kubernetes grows and becomes more diverse.  It&#x27;s the kube-scheduler that handles this task.</p><p>Users are able to set pod priority, which allows other less important pods to be evicted if need be so that the higher priority pod can be scheduled.  The scheduler will keep track of the nodes in your cluster, filters them, and then uses priority functions to determine the best node to schedule a pod on.  Once determines, the pod specification is sent to that node&#x27;s kubelet to be created.</p><p>Scheduling decisions can be influenced by using labels on either the node or pod.  Labels for podAffinity, taints, and bindings allow more configuration of how a pod is assigned to a node. These labels do not have to be drastic either, they can serve the function, for example, of stating a preference of a Pod one where to be scheduled, but if that can not occur it could be scheduled somewhere else.  We will oftentimes see <em>require</em> used in the name of parameters, but in reality this is more of a preference. Some setting will also evict pods if the conditions are no longer met (e.g. <code>requiredDuringScheduling</code> or <code>requiredDuringExecution</code>).</p><p>Custom schedulers can also be configured but need to be added to the cluster manually.</p><h3 id="filtering-predicates">Filtering (predicates)</h3><p>The scheduler runs through a list of filters, or <strong>predicates</strong>, to find which nodes are available, and then ranks the nodes using priority functions.  The node that ranks the highest is chosen by the scheduler. </p><p>The following are the predicates used:</p><ul><li>PodFitsHostPorts</li><li>PodFitsHost</li><li>PodFitsResources</li><li>MatchNodeSelector</li><li>NoVolumeZoneConflict</li><li>NoDiskConflict</li><li>MaxCSIVolumeCount</li><li>PodToleratesNodeTaints</li><li>CheckVolumeBinding</li></ul><p>The predicates are evaluated in an order, which can be configured. This allows for the least amount of checks possible for the scheduler.</p><p>These predicates work to filter out the nodes for certain conditions. For example, the <code>PodFitsResources</code> would filter out any nodes that do not have the required resources for the pod pending creation.</p><p>Prior to v1.23 you could use a <code>kind: Policy</code> to order and give weight to certain predicates. Policies are now depreciated.</p><h3 id="scoring-priorities">Scoring (Priorities)</h3><p>Priorities are used to weight resources. Unless the SelectorSpreadPriority (ranks nodes based on number of existing running pods) setting has been configured with pod and node affinity, the node with the least amount of pods will be chosen, which is a basic way of spreading pods about the cluster.</p><p>Different priorities are used depending on cluster needs. For example, you could use an <code>ImageLocalityPriority</code> to favor nodes that have a container&#x27;s image already downloaded.</p><p>The list of priorities that can be used are:</p><ul><li>SelectorSpreadPriority</li><li>InterPodAffinityPriority</li><li>LeastRequestedPriority</li><li>MostRequestedPriority</li><li>RequestedToCapacityRatioPriority</li><li>BalancedResourceAllocation</li><li>NodePreferAvoidPodsPriority</li><li>NodeAffinityPriority</li><li>TaintTolerationPriority</li><li>ImageLocalityPriority</li><li>ServiceSpreadingPriority</li><li>EqualPriority</li><li>EvenPodsSpreadPriority</li></ul><p>More detail on what each of these priorities do can be <a href="https://v1-22.docs.kubernetes.io/docs/reference/scheduling/policies/#priorities">found here</a>.</p><p>Prior to v1.23 you could use a <code>kind: Policy</code> to specify the priorities to use in scoring.  As noted before, this is depreciated.</p><h3 id="scheduling-policies">Scheduling Policies</h3><div class="admonition admonition-important alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>important</h5></div><div class="admonition-content"><p>Scheduling policies were <a href="https://kubernetes.io/docs/reference/scheduling/policies/">depreciated after v1.23</a> in favor of scheduler configuration so newer versions will not have this capability. See more on scheduler configuration <a href="https://kubernetes.io/docs/reference/scheduling/config/">here</a>.</p></div></div><p>By default, the scheduler has predicates and priorities it uses; these can be changed with scheduler policies however.</p><pre><code class="language-json">{
    &quot;kind&quot; : &quot;Policy&quot;,
    &quot;apiVersion&quot; : &quot;v1&quot;,
    &quot;predicates&quot; : [
        {&quot;name&quot; : &quot;MatchNodeSelector&quot;, &quot;order&quot;: 6},
        {&quot;name&quot; : &quot;PodFitsHostPorts&quot;, &quot;order&quot;: 2},
        {&quot;name&quot; : &quot;PodFitsResources&quot;, &quot;order&quot;: 3},
        {&quot;name&quot; : &quot;NoDiskConflict&quot;, &quot;order&quot;: 4},
        {&quot;name&quot; : &quot;PodToleratesNodeTaints&quot;, &quot;order&quot;: 5},
        {&quot;name&quot; : &quot;PodFitsHost&quot;, &quot;order&quot;: 1}
    ],
    &quot;priorities&quot; : [
        {&quot;name&quot; : &quot;LeastRequestedPriority&quot;, &quot;weight&quot; : 1},
        {&quot;name&quot; : &quot;BalancedResourceAllocation&quot;, &quot;weight&quot; : 1},       
        {&quot;name&quot; : &quot;ServiceSpreadingPriority&quot;, &quot;weight&quot; : 2},
        {&quot;name&quot; : &quot;EqualPriority&quot;, &quot;weight&quot; : 1}   
    ],
    &quot;hardPodAffinitySymmetricWeight&quot; : 10
}
</code></pre><p>Typically, you would use the <code>--policy-config-file</code> to pass the files and the <code>--scheduler-name</code> to give the scheduler a name. You&#x27;d then have two schedulers running and can specify which to use in the pod specification.</p><p>Having multiple schedulers opens up the potential for conflict in allocation of pods.  Each pod should declare which scheduler it should use, but if not and separate schedulers determine that a node is eligible, and both deploy to it and then cause the resources to no longer exist, then a conflict occurs. The local kubelet would then return these pods to the scheduler to reassign until eventually one succeeds and another node is chosen for the other pod.</p><h3 id="pod-specification">Pod specification</h3><p>Most scheduling decisions are made off what&#x27;s defined in the podSpec.  The fields that carry much of the weight for this are:</p><h4 id="nodename-and-nodeselector"><code>nodeName</code> and <code>nodeSelector</code></h4><p>These options allow a Pod to be assigned to a single node or a node with a certain label.</p><h4 id="affinity"><code>affinity</code></h4><p>Affinity and anti-affinity can be used to set a requirement or preference for which nodes to schedule to.  If using a preference, the matching node is chosen first, but other nodes are used if no match is found.</p><h4 id="tolerations"><code>tolerations</code></h4><p>Taints are placed on nodes so pods are not scheduled there for whatever reason.  Tolerations allow pods to ignore the taint and be scheduled on that node as long as other requirements for scheduling are met.</p><h4 id="schedulername"><code>schedulerName</code></h4><p>If you want to define the use of a custom scheduler, you could use the <code>schedulerName</code> field to do so.</p><h3 id="specifying-the-node-label">Specifying the node label</h3><p>The <code>nodeSelector</code> field is a straightforward way to to target a nde or set of nodes for scheduling.  It used one or more key-value labels.</p><p>For example:</p><pre><code class="language-yaml">spec:
  containers:
  - name: redis
    image: redis
  nodeSelector:
    net: fast
</code></pre><p>In the example we are using the <code>nodeSelector</code> to target any nodes with a <code>net</code> label and value of <code>fast</code>. Reminder that these labels and values are administrator created and are not actually necessarily tied to any properties of the hardware of the node.</p><p>A pod remains in a pending state until a node is found with matching labels.</p><p>You should be able to use node affinity to do the same thing as a <code>nodSelector</code>.</p><h3 id="scheduler-profiles">Scheduler profiles</h3><p>The scheduler can also be configured via scheduling profiles. They allow for configuration of <strong>extension points</strong> where plugins can be used.  An extension point is one of the 12 stages of scheduling, where a plugin can be used to modify the of a scheduler.  The 12 are:</p><ul><li>queueSort</li><li>perFilter</li><li>filter</li><li>postFilter</li><li>preScore</li><li>score</li><li>reserve</li><li>permit</li><li>preBind</li><li>bind</li><li>postBind</li><li>multiPoint</li></ul><p>Many plugins are enabled by default, and others can be enabled, to change how the scheduler chooses a node for a give podSpec.  The current plugin list can be <a href="https://kubernetes.io/docs/reference/scheduling/config/#profiles">found here</a>.</p><p>A scheduler can have multiple profiles enabled, which potentially removes the need of having multiple schedulers.Each podSpec would need to declare a profile to use, otherwise the <code>default-scheduler</code> is used if none is configured.</p><p>Profiles are what have replaced policies.</p><h3 id="pod-affinity-rules">Pod affinity rules</h3><p>An example of affinity may be desiring two pods to be co-located since they often communicate and share data.</p><p>An example of anti-affinity would be wanting two pods running on separate nodes to increase fault tolerance.</p><p>These settings are used by the scheduler to look at the pods currently running and their labels. The scheduler then must interrogate each node and keep track of the labels on running nodes, and because of this, clusters larger than several hundred nodes can see significant performance degradation.  Pod affinity rules use <code>In</code>, <code>NotIn</code>, <code>Exists</code>, and <code>DoesNotExist</code> operators.</p><h4 id="requiredduringschedulingignoredduringexecution"><code>requiredDuringSchedulingIgnoredDuringExecution</code></h4><p>The pod will not be scheduled to a node unless the operator is true.  If the operator becomes false in the future, the pod continues to run. This can be seen as a hard setting.</p><h4 id="preferredduringschedulingignoredduringexecution"><code>preferredDuringSchedulingIgnoredDuringExecution</code></h4><p>Similar to <code>requiredDuringSchedulingIgnoredDuringExecution</code>, but if no node with the desired setting is present, it will be scheduled anyway.  This declares a preference rather than requirement.</p><h4 id="podaffinity"><code>podAffinity</code></h4><p>This is used to schedule pods together on the same node.</p><h4 id="podantiaffinity"><code>podAntiAffinity</code></h4><p>And this is used to keep pods from being scheduled on the same node.</p><pre><code class="language-yaml">spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
</code></pre><p>Here is an example of pod affinity where a specific label needs to be matched for the Pod to be scheduled and started.  However, this label could be removed in the future and the Pod would still run.</p><p>If no node is found with another pod running with a <code>security=S1</code> label, then the pod remains unscheduled and <code>pending</code> until the condition is met.</p><p>In the pod anti affinity example, it is preferred that a node with no pods matching a <code>security=S1</code> label be found.  Also, because this is a preference, the pod will still run, but we are able to provide a weight to influence the scheduler.  Weights can be from 1 to 100.</p><h3 id="node-affinity-rules">Node affinity rules</h3><p>Aside from pod to pod affinity, we can also have pod to node affinity, which is managed by <code>nodeAffinity</code>.  It is similar to <code>nodeSelector</code> and should one day replace it in later versions of the API.  Instead of labels on other pods, <code>nodeAffinity</code> uses the labels on the nodes, so there is much less performance impact than with pod affinity.</p><p>Node affinity also uses the operators <code>In</code>, <code>NotIn</code>, &#x27;Exists<code>and</code>DoesNotExist<code>. </code>requiredDuringSchedulingIgnoredDuringExecution<code>and</code>preferredDuringSchedulingIgnoredDuringExecution` are also applicable for node affinity.</p><pre><code class="language-yaml">spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: discspeed
            operator: In
            values:
            - quick
            - fast
</code></pre><p>Here in our example, our pod to be scheduled will prefer nodes that have a label with key <code>discspeed</code> and a value of <code>fast</code> or <code>quick</code>.</p><h3 id="taints">Taints</h3><p>Taints are the opposite of node affinity.</p><p>To repel a pod from being deploy on a node you can set a taint. Pod will not be scheduled to that node unless they have the proper toleration, nor will the node even be considered. Taints are configured as <code>key=value:effect</code>. Any valid <code>key</code>, <code>value</code> label can be used for the taint. </p><p>There are three kinds of effects that can be applied for a taint. For any node that has multiple taints, the scheduler just ignores the ones with matching tolerations and allow the others to still take effect.  </p><h4 id="noschedule"><code>NoSchedule</code></h4><p>A <code>NoSchedule</code> effect will not schedule the pod on a matching node, unless the Pod has a toleration of this type.  Existing pods will continue to run regardless of toleration.</p><h4 id="prefernoschedule"><code>PreferNoSchedule</code></h4><p>A &quot;soft&quot; version of <code>NoSchedule</code>. The node with this taint will be avoided if possible. It is a preference, rather than requirement.</p><h4 id="noexecute"><code>NoExecute</code></h4><p>This taint will evacuate all existing pods and not allow future pods to be scheduled, except those with the correct toleration.  Pods with a <code>tolerationSeconds</code> set will wait however many configured seconds before eviction. Certain issues on a node may cause the kubelet to add 300 seconds tolerations to prevent unnecessary evictions.</p><h3 id="tolerations-1">Tolerations</h3><p>Tolerations can be set on a pod to negate the effect of a taint on the node.</p><p>Tolerations can take two different operators, <code>Equal</code> and <code>Exists</code>. <code>Equal</code> is the default if an operator is not declared.</p><p>Here&#x27;s two examples of tolerations.</p><pre><code class="language-yaml" metastring="title=equal-toleration.yaml" title="equal-toleration.yaml">tolerations:
- key: &quot;region&quot;
  operator: &quot;Equal&quot;
  value: &quot;us-east-1&quot;
  effect: &quot;NoExecute&quot;
  tolerationSeconds: 3600
</code></pre><pre><code class="language-yaml" metastring="title=exists-toleration.yaml" title="exists-toleration.yaml">tolerations:
- key: &quot;example-key&quot;
  operator: &quot;Exists&quot;
  effect: &quot;NoSchedule&quot;
</code></pre><p>In the <code>Equal</code> example, the pod will tolerate any nodes with a <code>NoExecute</code> taint for 3600 seconds and a label of <code>region=us-east-1</code>.</p><p>In the <code>Exists</code> example, no value is taken, as the <code>Exist</code> just looks for the existence of the label key.  If no key was specified, the pod would tolerate all nodes with the configured effect.</p><h3 id="custom-scheduler">Custom scheduler</h3><p>If the affinity and taints are not flexible enough you can write a custom scheduler. This is outside the scope of the fundamentals of Kubernetes, but a good starting point is the <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler">existing scheduling code</a>.</p><p>If a podSpec does not declare the scheduler to use, then the default is chosen. If the pod declares a scheduler that is not running, it would sit as <code>Pending</code> forever.</p><p>At the end of scheduling, the pod gets a binding specifying which pod it should run on.  Bindings are Kubernetes primitives in the <code>api/v1</code> group.  You could technically still schedule a pod without a scheduler running by defining a binding yourself.</p><p>Multiple schedulers can be running at the same time. The scheduler can be views along with other information by running <code>kubectl get events</code>.</p><h2 id="lab-exercises">Lab Exercises</h2><h3 id="lab-121---assign-pods-using-labels">Lab 12.1 - Assign pods using labels</h3><p>This lab will walk through scheduling pods on specific nodes using labels.  While it is normally best to let the system distribute pods for you, sometimes there may be exceptions to this.  For example, you may want a pod to run on a certain node due to hardware considerations.</p><p>First, let&#x27;s get our list of nodes and view their current labels and taints.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl get nodes</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe nodes | grep -A5 -i labels</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe nodes |grep -i taint</p></div></div><p>Find the count of how many pods are running on each node. Take node of the difference.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>For this exercice, we will make the cp node exclusive for VIP pods, and the worker for everything else. Then verify your new labels.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl label nodes &lt;cp node name&gt; status=vip</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl label nodes &lt;worker node name&gt; status=other</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl get nodes --show-labels</p></div></div><p>Now, let&#x27;s create a new Pod running four <code>busybox</code> containers. We&#x27;ll use <code>nodeSelector</code> to choose the node we want to deploy on.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ vim vip.yaml</p></div></div><pre><code class="language-yaml" metastring="title=&quot;vip.yaml&quot;" title="&quot;vip.yaml&quot;">apiVersion: v1
kind: Pod
metadata:
  name: vip
spec:
  containers:
  - name: vip1
    image: busybox
    args:
    - sleep
    - &quot;1000000&quot;
  - name: vip2
    image: busybox
    args:
    - sleep
    - &quot;1000000&quot;
  - name: vip3
    image: busybox
    args:
    - sleep
    - &quot;1000000&quot;
  - name: vip4
    image: busybox
    args:
    - sleep
    - &quot;1000000&quot;
  nodeSelector:
    status: vip
</code></pre><p>Then deploy the Pod and verify it was deployed on the cp node. You should see an additional pod running on the cp and no change on the worker.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl create -f vip.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>Delete the pod, comment out <code>nodeSelector</code>, then recreate the deployment. The Pod should be able to schedule on either node now.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl delete pod vip</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ vim vip.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl create -f vip.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>Copy the <code>vip.yaml</code> file, replace references to <code>vip</code> with <code>other</code> and add back the <code>nodeSelector</code>.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ cp vip.yaml other.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ sed -i s/vip/other/g other.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ vim other.yaml</p></div></div><p>Then create the new pod and verify where they get deployed.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl create -f other.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>Then delete the pods.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl delete pod vip other</p></div></div><h3 id="lab-122---using-taints-to-control-pod-deployment">Lab 12.2 - Using taints to control pod deployment</h3><p>First, create a new deployment and deploy it.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ vim taint.yaml</p></div></div><pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: taint-deployment
spec:
  replicas: 8
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.20.1
        ports:
        - containerPort: 80
</code></pre><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl apply -f taint.yaml</p></div></div><p>Then determine where the pods for the deployment were scheduled.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>You&#x27;ll likely see that some of the pods were sent to one or the other node. Now let&#x27;s delete the deployment, verify the pods are gone, and apply a taint to force where the pods will be deployed. Then we will redeploy the <code>taint.yaml</code> deployment.</p><p>Remember that the taints related to scheduling will have no effect on already running pods, while a <code>NoExecute</code> taint will evict currently running pods unless they have a matching toleration.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl delete deployment taint-deployment</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba=value:PreferNoSchedule</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node | grep Taint</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl apply -f taint.yaml</p></div></div><p>Verify the pods are up and running, the check where they were deployed to. Once you verify where the pods were deploy to, delete the deployment again.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl get pods</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl delete deployment taint-deployment</p></div></div><p>Now let&#x27;s untaint the node and apply a new one, this time using the <code>NoSchedule</code> effect. Then we will redeploy and check where the pods where scheduled.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba-</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba=value:NoSchedule</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl apply -f taint.yaml</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>Finally, we will use the <code>NoExecute</code> effect.  Do the same process of untainting, then retainting, and then verify the number of pods running per node.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba-</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba=value:NoExecute</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p></div></div><p>Finally remove that taint and see how the pods are reshuffled again. They will likely not be distributed the same as they were originally. Once done, delete the deployment for good.</p><div style="border-radius:10px"><div style="display:flex;background-color:#edeeef;padding:6px;border-top-left-radius:10px;border-top-right-radius:10px"><div style="background-color:#FF605C;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#FFBD44;border-radius:100%;width:10px;height:10px;margin:3px"></div><div style="background-color:#00CA4E;border-radius:100%;width:10px;height:10px;margin:3px"></div></div><div style="background-color:black;color:#00ff01;font-family:monospace;border-bottom-left-radius:10px;border-bottom-right-radius:10px;padding-top:5px;padding-bottom:5px;padding-left:4px;margin-bottom:10px"><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl taint nodes &lt;worker node name&gt; bubba-</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;cp node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl describe node &lt;worker node name&gt; | grep &quot;Non-terminated Pods&quot;</p><p style="margin:0px;padding:2px">ubuntu@cp:~ $ kubectl delete deployment taint-deployment</p></div></div><h2 id="knowledge-check">Knowledge check</h2><ul><li>Multiple scheduler <strong>can</strong> be run at the same time</li><li>Labels and annotations are <strong>not</strong> used for the same purpose</li><li>When a node is tainted, a pod requires a <strong>toleration</strong> to be deployed to that node</li><li><strong>Not</strong> all taints cause pods to not run on a node</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/fundamentals/chapter12.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/cka-prep/docs/fundamentals/chapter11"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Ingress</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/cka-prep/docs/fundamentals/chapter13"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Logging and Troubleshooting</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#course-reading" class="table-of-contents__link toc-highlight">Course Reading</a><ul><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning objectives</a></li><li><a href="#kube-scheduler" class="table-of-contents__link toc-highlight">kube-scheduler</a></li><li><a href="#filtering-predicates" class="table-of-contents__link toc-highlight">Filtering (predicates)</a></li><li><a href="#scoring-priorities" class="table-of-contents__link toc-highlight">Scoring (Priorities)</a></li><li><a href="#scheduling-policies" class="table-of-contents__link toc-highlight">Scheduling Policies</a></li><li><a href="#pod-specification" class="table-of-contents__link toc-highlight">Pod specification</a></li><li><a href="#specifying-the-node-label" class="table-of-contents__link toc-highlight">Specifying the node label</a></li><li><a href="#scheduler-profiles" class="table-of-contents__link toc-highlight">Scheduler profiles</a></li><li><a href="#pod-affinity-rules" class="table-of-contents__link toc-highlight">Pod affinity rules</a></li><li><a href="#node-affinity-rules" class="table-of-contents__link toc-highlight">Node affinity rules</a></li><li><a href="#taints" class="table-of-contents__link toc-highlight">Taints</a></li><li><a href="#tolerations-1" class="table-of-contents__link toc-highlight">Tolerations</a></li><li><a href="#custom-scheduler" class="table-of-contents__link toc-highlight">Custom scheduler</a></li></ul></li><li><a href="#lab-exercises" class="table-of-contents__link toc-highlight">Lab Exercises</a><ul><li><a href="#lab-121---assign-pods-using-labels" class="table-of-contents__link toc-highlight">Lab 12.1 - Assign pods using labels</a></li><li><a href="#lab-122---using-taints-to-control-pod-deployment" class="table-of-contents__link toc-highlight">Lab 12.2 - Using taints to control pod deployment</a></li></ul></li><li><a href="#knowledge-check" class="table-of-contents__link toc-highlight">Knowledge check</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/cka-prep/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/andrewdaoust/cka-prep" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2022 Andrew D'Aoust. Built with Docusaurus.</div></div></div></footer></div>
<script src="/cka-prep/assets/js/runtime~main.c69fe18d.js"></script>
<script src="/cka-prep/assets/js/main.32f4f9d5.js"></script>
</body>
</html>